[
  {
    "objectID": "talks.html",
    "href": "talks.html",
    "title": "Talks",
    "section": "",
    "text": "Talks conducted"
  },
  {
    "objectID": "projects/index.html",
    "href": "projects/index.html",
    "title": "Projects",
    "section": "",
    "text": "Brush Selection Tool for QGIS\n\n\n\nSep 9, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nImage Viewer (Streetview-Alike)\n\n\n\nSep 6, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nNon Local Means Visualisation\n\n\n\nApr 18, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nSurface Water Extent using Sentinel-2 Imagery\n\n\n\nFeb 25, 2025\n\n\n\n\n\n\n\n\n\n\n\n\n\nTrain 1Spacenet 5 CRESI Algorithm to extract road network from Satellite Imagery.\n\n\n\nJul 28, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\nSMAP Soil Moisture Time Series Analysis\n\n\n\nSep 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpenlayers Query via Geoserver\n\n\n\nAug 10, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nPredicting Age group of Social Media Application users\n\n\n\nJul 24, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nStructural Modelling and analysis of concrete canoe hull\n\n\n\nJul 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nAnalyse professional degree student enrolment v/s university count\n\n\n\nJul 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nLand Surface Temperature\n\n\n\nJul 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nRadiant Earth Spot the Crop Hackathon\n\n\n\nJul 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nWeb application to access store information on click of marker!\n\n\n\nJul 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nDjango-rest-framework CRUD Token Authetication Application\n\n\n\nJul 12, 2021\n\n\n\n\n\n\n\n\n\n\n\n\n\nGeocoding using Mapbox API\n\n\n\nJul 8, 2021\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/atharva/index.html",
    "href": "projects/atharva/index.html",
    "title": "Structural Modelling and analysis of concrete canoe hull",
    "section": "",
    "text": "Hello hello world"
  },
  {
    "objectID": "presentations/foss4g_asia_2023/index.html",
    "href": "presentations/foss4g_asia_2023/index.html",
    "title": "Sentinel-1 Image Analysis: Flood Detection using Cloud Native Tooling",
    "section": "",
    "text": "Hello hello world"
  },
  {
    "objectID": "presentations/datajam_2023/index.html",
    "href": "presentations/datajam_2023/index.html",
    "title": "Structural Modelling and analysis of concrete canoe hull",
    "section": "",
    "text": "Hello hello world"
  },
  {
    "objectID": "posts/understanding-jason-3-satellite-data-and-its-applications/index.html",
    "href": "posts/understanding-jason-3-satellite-data-and-its-applications/index.html",
    "title": "Understanding Jason-3 satellite: Data and its Applications",
    "section": "",
    "text": "What is Jason-3?\nJason-3 is a satellite which measures topographic height of the entire earth once every ~10 days since the year 2016 and is used in applications such as sea level rise, ocean circulation, and climate change. - It has an altimeter which measures the two-way travel time from the Earth’s surface to satellite. - It emits a pulse (radar pulse in this case) at a certain frequency to measure time. Thus it can also penetrate clouds.\n\n\n\n\nFig.1 - Jason-3 ground track visualised on felt. Each point represents a measurement. The distance between two measurements depends on specific product type. \n\n\n\nThe data format of this satellite is Vector Points, distributed as netCDF4 file format.\n\n\n\nUnderstanding Jason-3 Family of products\nJason-3 is processed to level-2 from telemetry data (level-0) and is available online for users to download via NOAA, EUMETSAT and CNES. Jason-3 level-2 product has 3 family of products depending on their latency. The near real time data with latency of 3-5 hours is categorised under Operational Geophysical Data Record (OGDR) product family, with latency of 1-2 days under Interim geophysical data record (IGDR) product family, and with latency of 60 days under geophysical data record (GDR) product family.\n\nNote: Higher the latency, more accurate the measurements since GDR products, unlike OGDR and IGDR products, are fully validated and calibrated.\n\nUnder each family, they are categorised with reduced, native and sensor product. The difference between them is in the amount and type of data included, as described in the figure below.\n\n\n\nDescribing Jason-3 Product Family and its childs. Reduced product only contains data with 1Hz frequency. Native product contains data with 1Hz + 20Hz. Sensor contains 1Hz + 20Hz + waveforms. GDR product is fully validated and has highest accuracy.\n\n\n\n\nWhich Jason-3 data product should I use?\nThe answer depends on 3 key factors: 1. Latency, spatial resolution, spectral resolution.\nI’ve already mentioned the latency difference between three parent products (see above section). Depending on user requirements and accuracy, one of the three families can be selected.\nLet us now look at two other factors which will help you decide which data product to choose.\nSpatial resolution here is the distance between two measurements, while spectral resolution is the richness of data. sensor product contains information about the photon signals (waveforms) and might not be useful for certain applications, while reduced product has sparsely spaced measurements, i.e, number of total measurements are less and it does not contain waveform information.\nTo illustrate which product to choose among the 8 products from 3 family, let us look at few real world use cases to help with our selection -\n\nIf I aim to create a DEM for a large scale area which has relatively relatively terrain surface and do not require near real time data, I’d prefer using reduced product from GDR family. While if the surface is undulating, I’d prefer native product, which has measurements at better spatial resolution.\nIf I am working on a climate variable, say, looking at atmospheric correction which requires raw photon signals as well, I’d use sensor product from IGDR family or GDR family depending on requirement.\nIf I am working on mission critical problem, and require near real time data with good spatial resolution, but do not need waveform data, I’d use native product from OGDR family.\n\n\n\nEarthdata hosts a product called GPS OGDR SSHA which is delivered near real time (8-10 hours) as reduced product (1Hz only). This product is more accurate than OGDR SSHA from the OGDR family described above due to it being processed against GPS orbit rather than DORIS orbit ( which is used for all other products described in figure 2). Though GPS OGDR SSHA product is only available from 2020 october onwards.\n\nNow that we know which product to download, let us look how and where to download them.\nManual Download\n\nEarthdata Search JASON3 GPS OGDR: It only allows you to visualise the ground track of the product you are going to download.\nNCEI NOAA JASON3: It has a GUI to check out the available products for each family, cycle and pass. Each family is suffixed with the version number.\nCMR API virtual Directory JASON3 GPS OGDR: It is same as NCEI NOAA, but only 1 product is available through CMR API virtual directory\n\nProgrammatic download Links\n\nPODAAC S3 access: Direct S3 access to only GPS OGDR SSHA reduced product. To know about bucket information, see here\nCMR API JASON3 GPS OGDR: CMI API access to only GPS OGDR SSHA reduced product. More details on how to use this API is described below.\nFTP NCEI NOAA JASON3: This FTP server hosts all the data described in figure 2. More information on how to download it is given below.\n\n\nData hosted by NCEI NOAA has all the data available for download, but NASA earthdata only hosts GPS OGDR SSHA reduced product.\n\n\n\nDownload from CMR API\nThe code for downloading jason-3 data using CMR API can be found here. Broadly, The code sends a request to CMR Search API https://cmr.earthdata.nasa.gov/search/granules.json along with parameters to filter by date, region and number of products required. The result is passed to authenticate with earthdata credentials and the file is downloaded to the local machine.\n\n\nDownload from FTP server\nThe entire code for downloading jason-3 data using FTP Server can be found here. In this code, the FTP directory is fetched for a specific product. To know which parent family folder to choose, we need to understand family versions for the Jason-3 product.\n\n\n\nJason-3 products grouped according to model version as seen in NOAA server directory at ncei.noaa.gov/data/oceans/jason3/.\n\n\nThe Jason-3 family product is versioned based on whether the product is in calibration/validation phase or intended to be used by the end user. If T is suffixed with the name of the parent folder, it means cal/val phase otherwise it is for end user. Product families with no suffix, combines all the versioned family product, keeping the latest for each cycle. It has data for all cycles till the latest available date.\n\nSince September 2018, all data products associated with gdr family have been moved to version f. If you require historcal jason3 data, it is a no-brainer to use GDR family products since it is at the most accurate among all the families and also has been updated with the latest model version f.\n\nReferences\n\nJason-3 Product Handbook\nPODAAC JPL NASA"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Blog",
    "section": "",
    "text": "My first FOSS4G experience\n\n\n\n\n\n\nfoss4g\n\n\n\nI traveled to Prizren, Kosovo to attend the largest geospatial conference in the world. This was my first international travel and conference!\n\n\n\n\n\n5 Jul 2023\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nUnderstanding Jason-3 satellite: Data and its Applications\n\n\n\n\n\n\nPython\n\nRemote Sensing\n\n\n\nUnderstand Jason-3 level-2 satellite products and know how to download it\n\n\n\n\n\n26 Jan 2023\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nCloud Native Composite, Subset and Processing of Satellite Imagery with STAC and Stackstac\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n14 Jan 2023\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nFull Fledged CRUD application using DRF and Token Authentication\n\n\n\n\n\n\nDjango\n\n\n\nHow to perform Create, Read, Update and Delete operations in Django Rest Framework\n\n\n\n\n\n2 Jan 2023\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nHow hard can it be to create 30 Maps?\n\n\n\n\n\n\nQGIS\n\n\n\nCompiling maps I made for Map challenge in Nov of 2022\n\n\n\n\n\n21 Dec 2022\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nImport CSV and OSM data into PostgreSQL using ogr2ogr\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n18 Sept 2022\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nMerging Rasters using Rasterio\n\n\n\n\n\n\nPython\n\n\n\nUse pystac-client to fetch and merge data. Also, understand the merge functionality of rasterio.\n\n\n\n\n\n7 Aug 2022\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nDownload MODIS data using CMR API in Python\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n28 Jul 2022\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nDownload and preprocess NASA GPM IMERG Data using Python and wget\n\n\n\n\n\n\nPython\n\n\n\nIn this blog post we look into how to download precipitation data from NASA website and process it with xarray and/or wget.\n\n\n\n\n\n20 Apr 2022\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nHow to Upload Multiple Geotagged Images in Django\n\n\n\n\n\n\nDjango\n\n\n\nwe look into how to upload multiple geo-tagged/non-geotagged images to aws s3 using plain Django and postgresql as databbase.\n\n\n\n\n\n3 Apr 2022\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nCOGs as the Stand-in Replacement for GeoTIFFs\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n6 Mar 2022\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nHow to save Earth Engine Image directly to your local machine\n\n\n\n\n\n\nearth-engine\n\n\n\nIn this post I show a trick which can let you download upto 100 times larger size images, directly to your local machine. Spoiler: getRegion method plays a significant role to help accomplish this task.\n\n\n\n\n\n7 Feb 2022\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nVector tiles and Docker using pg_tilerserv\n\n\n\n\n\n\nDocker\n\nVector-tiles\n\n\n\nServe your geospatial data as Vector Tiles using pg_tileserv in a Docker container\n\n\n\n\n\n22 Dec 2021\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nPolygonize Raster and Compute Zonal-Statistics in Python\n\n\n\n\n\n\nGDAL\n\nPython\n\n\n\nComputing zonal statistics over raster using GDAL and Python\n\n\n\n\n\n30 Sept 2021\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nTwo ways to Programmatically change projection of raw CSV\n\n\n\n\n\n\nGDAL\n\nPython\n\n\n\nChange projection of CSV on-the-fly using GDAL, GeoPandas\n\n\n\n\n\n30 Sept 2021\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nOverlay cropped raster with vector layer\n\n\n\n\n\n\nRemote Sensing\n\nPython\n\n\n\nPlot cropped raster and vector layer on the same figure using rasterio and matplotlib\n\n\n\n\n\n19 Sept 2021\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nSMAP Time Series\n\n\n\n\n\n\nRemote Sensing\n\n\n\nWe perform time-series analysis of soil moisture derived from SMAP L3 product for Bengaluru city.\n\n\n\n\n\n8 Sept 2021\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nContour Maps in QGIS\n\n\n\n\n\n\nQGIS\n\n\n\nGenerate contour maps in QGIS and understand various interpolation methods.\n\n\n\n\n\n24 Jul 2021\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nQuery Geoserver Layer using openlayers\n\n\n\n\n\n\nGeoserver\n\nOpenLayers\n\n\n\nThis blog demonstrates how to display and query all geoserver layers or from a workspace using geoserver REST API. CQL (Common Query Language) filter provided by geoserver is used to query the layer.\n\n\n\n\n\n16 Jul 2021\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nValidating LULC classes in QGIS\n\n\n\n\n\n\nQGIS\n\nmachine learning\n\n\n\nThe objective of this quality assessment was to validate the land cover map performed on June, 2020 sentinel-2 imagery by k-means classification algorithm in QGIS\n\n\n\n\n\n9 Jun 2021\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nDjango rest framework PDF creation and email via gmail SMTP and reportLab\n\n\n\n\n\n\nDjango\n\n\n\nIn this blog we create PDF using Report Lab and email it to the user using gmail SMTP service. All actions are performed in Django.\n\n\n\n\n\n24 May 2021\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nPolygon Selection and Area Calculation in Openlayers\n\n\n\n\n\n\nOpenLayers\n\n\n\nSelect multiple polygons (parcels) and calculate area on the fly in openlayers\n\n\n\n\n\n24 May 2021\n\n\nAman Bagrecha\n\n\n\n\n\n\n\n\n\n\n\n\nGeocoding using Mapbox API with Zoom-in map functionality\n\n\n\n\n\n\nOpenLayers\n\nMapbox\n\n\n\nHow to bulid geocoding web-app using openlayers\n\n\n\n\n\n4 May 2021\n\n\nAman Bagrecha\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/first-foss4g/index.html",
    "href": "posts/first-foss4g/index.html",
    "title": "My first FOSS4G experience",
    "section": "",
    "text": "I embarked on a journey to attend the esteemed geospatial conference, FOSS4G 2023, held in Prizren. As I reflect, it was an event that left a profound impact on me, and I want to treasure these memories forever.\n\n\n\nEntering the exhibition center at ITP\n\n\nJust a couple of months ago, I had my doubts about attending FOSS4G, as I had never traveled to a foreign country before and the host country was not very well known. The logistics to get visa and planning the round-trip seemed daunting, and I wasn’t sure if it would be worth the investment and energy. However, to my delight, I discovered that there were other indians planning their travel too, making the planning process much smoother.\nSo, there I was, hopping on the plane to Pristina, Kosovo, and thinking to myself, “Did I really make the right call by shelling out a ton of money for this conference?” But, looking back now, I have to say, it was totally worth it! All the energy and effort I put into being a part of this event, it paid off big time. I’m filled with a sense of contentment right now.\nDuring my time at foss4g, I got to hang out with the cool, nerdy core developers, authors, organizers, professors, and users of geospatial technology. I made some awesome new friends and dove headfirst into the wild nightlife of that stunning city. The experience was overwhelming, and it demanded an extra surge of energy.\nWith dense knowledge delivered within every conference room, I had to decide to let go one talk over another. Thankfully the talks are recorded, and will be shared freely and openly, adhering the spirit of foss4g!\n\n\n\nDay5: Presenting my work to a room full of geonerds\n\n\nNot only was this my first foss4g, but also the first time I presented my work. I was thrilled to receive positive feedback after my talk and to connect with individuals who shared my passion and interests. It was like finding my tribe!\nThese ten days in Kosovo were one of the most remarkable days of my life. The local people I met, both from Kosovo and Albania, made me feel right at home and added to the incredible experience. I’m back home, with a fresh perspective on life, a bunch of goals to accomplish, and a trove of memories that I will cherish forever.\n\n\n\nRandom day: City tour with friends\n\n\nLittle did I know, a “mere” conference could dive deep into technical know-how, forge bonds for a lifetime, question my views on status quo, understand cultural differences, and also make me feel blessed to be part of this community!\nI cannot fathom the sheer knowledge I gained during these 10 days! From learning about state of the art innovation in geospatial, to learning life lessons to think beyond technology! I hope to have the opportunity to attend again next year and reunite with the friends I made along the way.\nThank you foss4g community, the people of Kosovo and to the organizers who made this extraordinary event possible!\nFingers crossed, hope to meet again next year in Brazil!\nPS: This post is a glimpse of what transpired during the conference! Highly encourage everybody to attend this event and experience it yourself!\n\n\n\nEveryday visit: Stunning landscape of Prizren city, Kosovo\n\n\n\n\n\nSaving some stickers for Lets Talk Spatial community! Grab them at next meetup :)"
  },
  {
    "objectID": "posts/2022-12-21-how-hard-can-it-be-to-create-30-maps/index.html",
    "href": "posts/2022-12-21-how-hard-can-it-be-to-create-30-maps/index.html",
    "title": "How hard can it be to create 30 Maps?",
    "section": "",
    "text": "I participated in the #30DayMapChallenge for the first time. I found the experience to be both challenging and rewarding wherein I was able to create a variety of maps using different techniques and tools. I got to use felt, blender, kepler, python and QGIS for my maps.\nThough I did not complete all of them, I realised how vital and hard it is to effectively communicate information through maps and how to use maps to make informed decisions. Hope to complete all 30 next year. Compiled all my maps for the year 2022 below -\nRead more about the challenge here\nDay1: Points\n\n\n\nday1-points\n\n\nDay2: Lines\n\n\n\nday2-lines\n\n\nDay3: Polygons \nDay4: Colour Friday: Green \nDay5: Ukraine\n\n\n\nday5-ukraine\n\n\nDay6: Network \nDay7: Raster \nDay8: Data: OpenStreetMap \nDay10: A bad map \nDay11: Colour Friday: Red \nDay12: Scale \nDay13: 5 minute map  Day14: Hexagons  Day16: Minimal \nDay21: Data: Kontur Population Dataset \nDay23: Movement \nDay25: Two Colors \nThe data I used for this challenge is archived on GitHub"
  },
  {
    "objectID": "posts/2022-07-31-merge-rasters-the-modern-way-using-python/index.html",
    "href": "posts/2022-07-31-merge-rasters-the-modern-way-using-python/index.html",
    "title": "Merging Rasters using Rasterio",
    "section": "",
    "text": "In this blog, we’ll examine how to merge or mosaic rasters using Python, the modern way. Additionally, we would look at a few nuances and internal workings of rasterio’s merge functionality along with saving your rasters in-memory.\nBy “modern way”, it is implied that you have an improved workflow and data management. And that you can experiment with various scenarios quickly and efficiently.\nThe traditional way to mosaic data is by downloading multiple intersecting tileset in its entirety. Downloading an entire tileset is itself a cost prohibitive task, added to already lost time in searching desired satellite imagery on GUI.\nTo overcome these traditional challenges, there has been significant improvement in storing metadata of satellite imagery (namely STAC) which has enabled querying them much smoother and made it imagery-provider agnostic.\n\nTL;DR\nWe would perform the following task in this blog — - Use pystac to query items over our AOI - Plot the tiles on map using hvplot - Merge tiles without data download on local machine - Save the merged tile in-memory using rasterio’s MemoryFile - Internals of rasterio’s merge methods\n\n\n\nProblem at hand\nI wish to access sentinel-2 True Color Image for the month of January over my area of interest (AOI), which is a highway network across Karnataka and Andhra Pradesh (Figure 1).\n\n\n\n\nbokeh_plot\n\n\n\nFig.1 - Highway Network as our Region of Interest&lt;/ href&gt; \n\n\nWe start by fetching sentinel-2 tiles over our AOI from sentinel-s2-l2a-cogs STAC catalog using pystac-client. This library allows us to crawl STAC catalog and enables rapid access to the metadata we need.\n# STAC API root URL\n# Thanks to element84 for hosting the API for sentinel-2 catalog.\nURL = 'https://earth-search.aws.element84.com/v0/'\n\nclient = Client.open(URL)\n\nsearch = client.search(\n    max_items = 10,\n    collections = \"sentinel-s2-l2a-cogs\",\n    bbox = gdf.total_bounds, # geodataframe for our region of study\n    datetime = '2022-01-01/2022-01-24'\n)\nIn the above code, we search for 10 sentinel-s2-l2a-cogs over our AOI for the date between January 1 and 24 of 2022.\nNow we need to know which of our 10 queried images covers our area of interest in its entirety. To do that, we can plot all the search results on the map and visually inspect.\n\n\n\n\nbokeh_plot\n\n\n\nFig.2 - Sentinel-2 tiles overlaid on Region of Interest&lt;/ href&gt; \n\n\nWe see that our AOI is not covered by a single tile in entirety, and that there is a need to merge adjacent tiles.\n\nNote that we have so far only queried the metadata of our desired imagery\n\nWe use rasterio’s merge functionality, which would enable us to combine all of them seamlessly.\nFirst, we get all the tiles for a single day and look for True Color Image (TCI) band\n# retrieve the items as dictionaries, rather than Item objects\nitems = list(search.items_as_dicts())\n# convert found items to a GeoDataFrame\nitems_gdf = items_to_geodataframe(items)\n\ntiles_single_day = items_gdf.loc['2022-01-23', \"assets.visual.href\"]\n\n# print(tiles_single_day)\n properties.datetime\n 2022-01-23 05:25:14+00:00    https://sentinel-cogs.s3.us-west-2.amazonaws.c...\n 2022-01-23 05:25:11+00:00    https://sentinel-cogs.s3.us-west-2.amazonaws.c...\n 2022-01-23 05:24:59+00:00    https://sentinel-cogs.s3.us-west-2.amazonaws.c...\n 2022-01-23 05:24:56+00:00    https://sentinel-cogs.s3.us-west-2.amazonaws.c...\n Name: assets.visual.href, dtype: object\nNext, read the remote files via the URL in the above output using rasterio.open and save the returned file handlers as a list. This is the first instance where we are dealing with the actual imagery. Although, we are not reading the values stored in the data just yet.\n# open images stored on s3\nfile_handler = [rasterio.open(row) for row in tiles_single_day]\nFinally we can merge all of the tiles and get the clipped raster stored in memory.\nfrom rasterio.io import MemoryFile\nfrom rasterio.merge import merge\n\nmemfile = MemoryFile()\n\nmerge(datasets=file_handler, # list of dataset objects opened in 'r' mode\n    bounds=tuple(gdf.set_crs(\"EPSG:4326\").to_crs(file_handler[0].crs).total_bounds), # tuple\n    nodata=None, # float\n    dtype='uint16', # dtype\n    resampling=Resampling.nearest,\n    method='first', # strategy to combine overlapping rasters\n    dst_path=memfile.name, # str or PathLike to save raster\n    dst_kwds={'blockysize':512, 'blockxsize':512} # Dictionary\n  )\nThere are really interesting things to look at in the above code. Overall, the code above returns a MemoryFile object which contains a uint16 raster with bounds of our AOI and blocksize of 512. The attribute dst_path allows us to specify a path to save the output as a raster. What is interesting is we can not only pass a file path to save on local disk but also a virtual path and save the merged raster in-memory, avoiding clutter of additional files on disk.\nTo define a virtual path, we use rasterio’s MemoryFile class. When we create a MemoryFile object, it has a name attribute which gives us a virtual path, thus treating it as a real file (using GDALs vsimem internally). This MemoryFile object (memfile here) provides us all the methods and attributes of rasterio’s file handler, which is extremely helpful.\nprint(memfile.open().profile)\n\n{'driver': 'GTiff', 'dtype': 'uint16', 'nodata': 0.0, 'width': 4110, 'height': 3211, 'count': 3, 'crs': CRS.from_epsg(32643), 'transform': Affine(10.0, 0.0, 788693.4700669964,\n       0.0, -10.0, 1500674.3670768766), 'blockxsize': 512, 'blockysize': 512, 'tiled': True, 'compress': 'deflate', 'interleave': 'pixel'}\nThe method='first' tells us the strategy used to determine the value of the pixel where the rasters overlap. In this case, the pixel value from the first imagery of the overlapping region in the list, is used as the value for the output raster.\nThe entire algorithm to merge rasters is illustrated in the figure below by taking an example of combining two rasters with method=first.\n\n\n\n\nmerge-rasterio-with-laberl_merging-rasters\n\n\n\nFig.3 - Internal working of rasterio’s merge functionality. src1 and src2 are two overlapping raster.&lt;/ href&gt; \n\n\nFrom the above figure, for each raster in the list: - it finds the intersection with the Output Bounds (named region in the figure)\n\nnext, it gets a boolean mask of invaild pixel over the region (named region_mask in the figure).\nnext, it copies over all the existing values from the raster for the region to an array (named temp in the figure)\nIt gets a boolean mask for the valid pixels in the temp array. (named temp_mask in the figure)\nWith these four arrays, it runs the method=first, which is to\n\ncreate the same shaped array as that of region and fill values with negation of region_mask (named A in the figure)\ncreate a filter by combining region_mask and A with a AND gate (named B in the figure)\ncopy over the values from temp to region using B as the filter\n\n\nThese series of steps are performed for all the rasters in the list. Finally, the output at the end of each iteration is combined to produce dest raster.\n\nNotice the dark strip bands for each array which represents the overlapping region. Also notice that values from the dark strip in step 1 did not change at the end of step 2\n\n\n\n\nCustom combining strategy for overlapping regions\nWe can have arbitrary conditions on how to combine the overlapping region. By default rasterio uses values of the first overlapping raster from the list of Input files as pixel values for the output raster file. It has several other options in its utility such as min, max, sum, count, last.\nTo define our custom method, say in this case, I want to take the average of all the pixel values over my overlapping region and copy them to the output file. To do that, we can override the method by defining our custom method. Let us see how —\nWe take a look at the source code of built-in methods which make use of two or more rasters to make decisions on the output pixel values. Few such methods which do that are copy_sum, copy_min, copy_max, copy_count.\nLooking at the copy_min from source code, we see that it performs two logical operations each before and after the custom logic we wish to apply.\n\n\n\n\nimage\n\n\n\nFig.4 - copy_min function copies minimum value from overlapping region to the output raster&lt;/ href&gt; \n\n\nWe would replace our custom logic of averaging with that of minimum in the above code and that is all there is to it. We can now use this function to manipulate the values of overlapping region!\ndef custom_method_avg(merged_data, new_data, merged_mask, new_mask, **kwargs):\n    \"\"\"Returns the average value pixel.\"\"\"\n    mask = np.empty_like(merged_mask, dtype=\"bool\")\n    np.logical_or(merged_mask, new_mask, out=mask)\n    np.logical_not(mask, out=mask)\n    np.nanmean([merged_data, new_data], axis=0, out=merged_data, where=mask)\n    np.logical_not(new_mask, out=mask)\n    np.logical_and(merged_mask, mask, out=mask)\n    np.copyto(merged_data, new_data, where=mask, casting=\"unsafe\")\n\n\n\nEndnote\nThe modern approach to merge rasters in python is to only stream the data for your region of interest, process and perform analysis on the raster in memory. This would save you a huge cost and time. This is possible because of COGs and STAC.\nWe looked at the merge method in depth and also explored the techniques used to combine the overlapping data. Finally, we created a custom method for merging rasters by modifying the existing code to suit our requirements. The code associated with this post can be found here."
  },
  {
    "objectID": "posts/2022-03-06-what-is-a-block-tile-overview-pyramids-and-cogs-an-experiment/index.html",
    "href": "posts/2022-03-06-what-is-a-block-tile-overview-pyramids-and-cogs-an-experiment/index.html",
    "title": "COGs as the Stand-in Replacement for GeoTIFFs",
    "section": "",
    "text": "I decided to write this blog when my twitter feed was buzzing with the usefulness of Cloud Optimized GeoTIFF (COGs) and how it is a paradigm shift in the way we serve raster on any client application. I also look at potential gotchas when creating COGs and when it might end up not being useful.\nSo, this post will mostly focus on COGs and why I would use them over plain GeoTIFFs. Also, we would look at associated jargons when you want to create a COG. I aim to dump my thoughts once and for all and hopefully help others on the way."
  },
  {
    "objectID": "posts/2022-03-06-what-is-a-block-tile-overview-pyramids-and-cogs-an-experiment/index.html#description",
    "href": "posts/2022-03-06-what-is-a-block-tile-overview-pyramids-and-cogs-an-experiment/index.html#description",
    "title": "COGs as the Stand-in Replacement for GeoTIFFs",
    "section": "description",
    "text": "description\n\n\n\nFilename\nSize\nRemarks\n\n\n\n\nT43PHP_20210123T051121_B03_20m.jp2\n33 Mb\nOriginal size\n\n\nb03.tif\n58 Mb\nconvert jp2 to tif\n\n\nb03.tif\n78 Mb\nadd overviews\n\n\nb03_cog.tif\n87 Mb\nconvert to COG\n\n\nb03_cog_deflate.tif\n57 Mb\nCOG with deflate\n\n\n\nBy this comparison you can see that, if you do not compress the file, it would ingress a huge cost to store them.\nIf your aim is to serve rasters on the browser or let users download the data, start using COGs with additional options and you won’t notice any difference but only save money in the long run."
  },
  {
    "objectID": "posts/2021-django-image-upload/index.html",
    "href": "posts/2021-django-image-upload/index.html",
    "title": "How to Upload Multiple Geotagged Images in Django",
    "section": "",
    "text": "In this post, we look into how to upload multiple geo-tagged/non-geotagged images to aws s3 using plain Django and spatialite as databbase. We use GeoDjango to store the latitude, longitude extracted from geo-tagged images into the database.\n\n\n\n\ncreate django project\ndjango-admin startproject login_boiler_plate\ncreate app python manage.py startapp GisMap\ncreate superuser python manage.py createsuperuser\nIn settings.py add the app to installed_app list and setup the default location for media storage.\nINSTALLED_APPS = [\n    ...\n    'GisMap',\n]\n\nMEDIA_ROOT =  os.path.join(BASE_DIR, 'media') \nMEDIA_URL = '/media/'\n\n\n\n# in settings.py file\nDATABASES = {\n    'default': {\n         'ENGINE': 'django.contrib.gis.db.backends.postgis', #imp\n         'NAME': 'database_name_here',\n         'USER': 'postgres',\n        'PASSWORD': 'password_here',\n        'HOST': 'localhost',\n        'PORT': '5432',\n    },\n}\nIn models.py, create model for uploading images. DateTimeField and user are not necessary.\nfrom django.db import models\nfrom django.contrib.auth.models import User\n\n\nclass ImageUpload(models.Model):\n    user = models.ForeignKey(User, null=True, on_delete=models.CASCADE)\n    image = models.ImageField( null=False, blank=False, upload_to = 'images/')\n    date_created = models.DateTimeField(auto_now_add=True, null=True)\n\n    def __str__(self):\n        return self.user.username + \" uploaded: \"+ self.image.name\nIn forms.py, refer to the ImageUpload model for input.\n  \nfrom django.forms import ModelForm\nfrom django.contrib.auth.models import User\nfrom .models import ImageUpload\n\nclass ImageForm(ModelForm):\n    class Meta:\n        model = ImageUpload\n        fields = ('image',)\nIn home.html, create the form to accept image upload.\n                  &lt;!-- Modal --&gt;\n                  &lt;form method = \"post\" enctype=\"multipart/form-data\"&gt;\n                  &lt;div class=\"modal fade\" id=\"exampleModal\" tabindex=\"-1\" role=\"dialog\" aria-labelledby=\"exampleModalLabel\" aria-hidden=\"true\" &gt;\n                    {% csrf_token %}\n                    &lt;div class=\"modal-dialog\" role=\"document\"&gt;\n                      &lt;div class=\"modal-content\"&gt;\n                        &lt;div class=\"modal-header\"&gt;\n                          &lt;h5 class=\"modal-title\" id=\"exampleModalLabel\"&gt;Upload Image&lt;/h5&gt;\n                          &lt;button type=\"button\" class=\"close\" data-dismiss=\"modal\" aria-label=\"Close\"&gt;\n                            &lt;span aria-hidden=\"true\"&gt;&times;&lt;/span&gt;\n                          &lt;/button&gt;\n                        &lt;/div&gt;\n                        &lt;div class=\"modal-body\"&gt;\n                          {{ image_form.image }}\n                        &lt;/div&gt;\n                        &lt;div class=\"modal-footer\"&gt;\n                          &lt;button type=\"button\" class=\"btn btn-secondary\" data-dismiss=\"modal\"&gt;Close&lt;/button&gt;\n                          &lt;button type=\"submit\" class=\"btn btn-primary\"&gt;Save Image&lt;/button&gt;\n                        &lt;/div&gt;\n                      &lt;/div&gt;\n                    &lt;/div&gt;\n                  &lt;/div&gt;\n                  &lt;/form&gt;\nIn views.py, accept the HTTP POST request and save to the database. We will alter this to extract latitude, longitude later.\n@login_required(login_url='login')\ndef home_page(request):\n\n    if request.method == 'POST':\n        form = ImageForm(request.POST , request.FILES)\n        print(form)\n        if form.is_valid():\n            print(\"is valid\")\n            obj = form.save(commit=False)\n            obj.user = request.user\n            obj.save()\n        return redirect('home')\n    else:\n        Imageform = ImageForm()\n        return render(request, \"GisMap/home.html\", {'Title': \"Home Page\", \"image_form\": ImageForm})\n\n\n\n\nGeodjango is built on top of django and adds spatial functionality such as storing points, lines , polygon and multipolygon. It is prepackaged with Django but requires few additional softwares to make it fully functional. These include- GDAL, PROJ, GEOS, PostGIS. These can be downloaded from osgeo4W which bundles all these libraries. Then application can be added to apps in settings with django.contrib.gis to the installed apps.\n\nBy default geodjango is not installed in the apps list and thus we do it ourself.\npip install django-geo\nNOTE- ensure os4geo is installed: install from here if not done. And make the following changes in settings.py.\nAn additional setting is required, which is to locate osgeo4w directory in django. If you install osgeo4w in default directory, you need to put the following code within the settings.py file.\nINSTALLED_APPS = [\n...\n    'django.contrib.gis',\n]\n\n\n\nimport os\nimport posixpath\nif os.name == 'nt':\n    import platform\n    OSGEO4W = r\"C:\\OSGeo4W\"\n    if '64' in platform.architecture()[0]:\n        OSGEO4W += \"64\"\n    assert os.path.isdir(OSGEO4W), \"Directory does not exist: \" + OSGEO4W\n    os.environ['OSGEO4W_ROOT'] = OSGEO4W\n    os.environ['GDAL_DATA'] = OSGEO4W + r\"\\share\\gdal\"\n    os.environ['PROJ_LIB'] = OSGEO4W + r\"\\share\\proj\"\n    os.environ['PATH'] = OSGEO4W + r\"\\bin;\" + os.environ['PATH']\nIn models.py, add a PointField which can store geospatial information (lat,lon)\nfrom django.contrib.gis.db import models\nclass ImageUpload():\n  ...  \n  geom = models.PointField( null=True)\nIn views.py, define functions to extract meta data from image and convert into right format for GeoDjango to understand it. Courtesy of Jayson DeLancey\n\n#________________________________________FUNCTIONS FOR IMAGE EXIF DATA______________________________________________________________________________#\n\n\n\nfrom PIL import Image\nfrom urllib.request import urlopen\nfrom PIL.ExifTags import GPSTAGS\nfrom PIL.ExifTags import TAGS\n\ndef get_decimal_from_dms(dms, ref):\n\n    degrees = dms[0]\n    minutes = dms[1] / 60.0\n    seconds = dms[2] / 3600.0\n\n    if ref in ['S', 'W']:\n        degrees = -degrees\n        minutes = -minutes\n        seconds = -seconds\n\n    return round(degrees + minutes + seconds, 5)\n\ndef get_coordinates(geotags):\n    lat = get_decimal_from_dms(geotags['GPSLatitude'], geotags['GPSLatitudeRef'])\n\n    lon = get_decimal_from_dms(geotags['GPSLongitude'], geotags['GPSLongitudeRef'])\n\n    return (lon, lat)\n\n\n\ndef get_geotagging(exif):\n    if not exif:\n        raise ValueError(\"No EXIF metadata found\")\n\n    geotagging = {}\n    for (idx, tag) in TAGS.items():\n        if tag == 'GPSInfo':\n            if idx not in exif:\n                raise ValueError(\"No EXIF geotagging found\")\n\n            for (key, val) in GPSTAGS.items():\n                if key in exif[idx]:\n                    geotagging[val] = exif[idx][key]\n\n    return geotagging\n\n#_______________________________________________________________________________________________________________________________________#\n\nIn views.py, update home_page function to extract meta data and save the image to database.\nfrom django.contrib.gis.geos import Point\n\n@login_required(login_url='login')\ndef home_page(request):\n    if request.method == \"POST\":\n        form = ImageForm(request.POST, request.FILES)\n        img = Image.open(request.FILES.get(\"image\"))\n        if form.is_valid():\n            try:\n                obj = form.save(commit=False)\n                obj.user = request.user\n                obj.image_url = obj.image.url\n                geotags = get_geotagging(img._getexif())\n                obj.geom = Point(\n                    get_coordinates(geotags)\n                )  # X is longitude, Y is latitude, Point(X,Y)\n                obj.save()\n                messages.success(request, f\"image uploaded succesfully\")\n            except ValueError as e:\n                messages.warning(request, e)\n        else:\n            messages.warning(request, f\"Invalid image type\")\n        return redirect(\"home\")\n    else:\n        Imageform = ImageForm()\n        return render(\n            request, \"GisMap/home.html\", {\"Title\": \"Home Page\", \"image_form\": ImageForm}\n        )"
  },
  {
    "objectID": "posts/2021-django-image-upload/index.html#overview",
    "href": "posts/2021-django-image-upload/index.html#overview",
    "title": "How to Upload Multiple Geotagged Images in Django",
    "section": "",
    "text": "In this post, we look into how to upload multiple geo-tagged/non-geotagged images to aws s3 using plain Django and spatialite as databbase. We use GeoDjango to store the latitude, longitude extracted from geo-tagged images into the database.\n\n\n\n\ncreate django project\ndjango-admin startproject login_boiler_plate\ncreate app python manage.py startapp GisMap\ncreate superuser python manage.py createsuperuser\nIn settings.py add the app to installed_app list and setup the default location for media storage.\nINSTALLED_APPS = [\n    ...\n    'GisMap',\n]\n\nMEDIA_ROOT =  os.path.join(BASE_DIR, 'media') \nMEDIA_URL = '/media/'\n\n\n\n# in settings.py file\nDATABASES = {\n    'default': {\n         'ENGINE': 'django.contrib.gis.db.backends.postgis', #imp\n         'NAME': 'database_name_here',\n         'USER': 'postgres',\n        'PASSWORD': 'password_here',\n        'HOST': 'localhost',\n        'PORT': '5432',\n    },\n}\nIn models.py, create model for uploading images. DateTimeField and user are not necessary.\nfrom django.db import models\nfrom django.contrib.auth.models import User\n\n\nclass ImageUpload(models.Model):\n    user = models.ForeignKey(User, null=True, on_delete=models.CASCADE)\n    image = models.ImageField( null=False, blank=False, upload_to = 'images/')\n    date_created = models.DateTimeField(auto_now_add=True, null=True)\n\n    def __str__(self):\n        return self.user.username + \" uploaded: \"+ self.image.name\nIn forms.py, refer to the ImageUpload model for input.\n  \nfrom django.forms import ModelForm\nfrom django.contrib.auth.models import User\nfrom .models import ImageUpload\n\nclass ImageForm(ModelForm):\n    class Meta:\n        model = ImageUpload\n        fields = ('image',)\nIn home.html, create the form to accept image upload.\n                  &lt;!-- Modal --&gt;\n                  &lt;form method = \"post\" enctype=\"multipart/form-data\"&gt;\n                  &lt;div class=\"modal fade\" id=\"exampleModal\" tabindex=\"-1\" role=\"dialog\" aria-labelledby=\"exampleModalLabel\" aria-hidden=\"true\" &gt;\n                    {% csrf_token %}\n                    &lt;div class=\"modal-dialog\" role=\"document\"&gt;\n                      &lt;div class=\"modal-content\"&gt;\n                        &lt;div class=\"modal-header\"&gt;\n                          &lt;h5 class=\"modal-title\" id=\"exampleModalLabel\"&gt;Upload Image&lt;/h5&gt;\n                          &lt;button type=\"button\" class=\"close\" data-dismiss=\"modal\" aria-label=\"Close\"&gt;\n                            &lt;span aria-hidden=\"true\"&gt;&times;&lt;/span&gt;\n                          &lt;/button&gt;\n                        &lt;/div&gt;\n                        &lt;div class=\"modal-body\"&gt;\n                          {{ image_form.image }}\n                        &lt;/div&gt;\n                        &lt;div class=\"modal-footer\"&gt;\n                          &lt;button type=\"button\" class=\"btn btn-secondary\" data-dismiss=\"modal\"&gt;Close&lt;/button&gt;\n                          &lt;button type=\"submit\" class=\"btn btn-primary\"&gt;Save Image&lt;/button&gt;\n                        &lt;/div&gt;\n                      &lt;/div&gt;\n                    &lt;/div&gt;\n                  &lt;/div&gt;\n                  &lt;/form&gt;\nIn views.py, accept the HTTP POST request and save to the database. We will alter this to extract latitude, longitude later.\n@login_required(login_url='login')\ndef home_page(request):\n\n    if request.method == 'POST':\n        form = ImageForm(request.POST , request.FILES)\n        print(form)\n        if form.is_valid():\n            print(\"is valid\")\n            obj = form.save(commit=False)\n            obj.user = request.user\n            obj.save()\n        return redirect('home')\n    else:\n        Imageform = ImageForm()\n        return render(request, \"GisMap/home.html\", {'Title': \"Home Page\", \"image_form\": ImageForm})\n\n\n\n\nGeodjango is built on top of django and adds spatial functionality such as storing points, lines , polygon and multipolygon. It is prepackaged with Django but requires few additional softwares to make it fully functional. These include- GDAL, PROJ, GEOS, PostGIS. These can be downloaded from osgeo4W which bundles all these libraries. Then application can be added to apps in settings with django.contrib.gis to the installed apps.\n\nBy default geodjango is not installed in the apps list and thus we do it ourself.\npip install django-geo\nNOTE- ensure os4geo is installed: install from here if not done. And make the following changes in settings.py.\nAn additional setting is required, which is to locate osgeo4w directory in django. If you install osgeo4w in default directory, you need to put the following code within the settings.py file.\nINSTALLED_APPS = [\n...\n    'django.contrib.gis',\n]\n\n\n\nimport os\nimport posixpath\nif os.name == 'nt':\n    import platform\n    OSGEO4W = r\"C:\\OSGeo4W\"\n    if '64' in platform.architecture()[0]:\n        OSGEO4W += \"64\"\n    assert os.path.isdir(OSGEO4W), \"Directory does not exist: \" + OSGEO4W\n    os.environ['OSGEO4W_ROOT'] = OSGEO4W\n    os.environ['GDAL_DATA'] = OSGEO4W + r\"\\share\\gdal\"\n    os.environ['PROJ_LIB'] = OSGEO4W + r\"\\share\\proj\"\n    os.environ['PATH'] = OSGEO4W + r\"\\bin;\" + os.environ['PATH']\nIn models.py, add a PointField which can store geospatial information (lat,lon)\nfrom django.contrib.gis.db import models\nclass ImageUpload():\n  ...  \n  geom = models.PointField( null=True)\nIn views.py, define functions to extract meta data from image and convert into right format for GeoDjango to understand it. Courtesy of Jayson DeLancey\n\n#________________________________________FUNCTIONS FOR IMAGE EXIF DATA______________________________________________________________________________#\n\n\n\nfrom PIL import Image\nfrom urllib.request import urlopen\nfrom PIL.ExifTags import GPSTAGS\nfrom PIL.ExifTags import TAGS\n\ndef get_decimal_from_dms(dms, ref):\n\n    degrees = dms[0]\n    minutes = dms[1] / 60.0\n    seconds = dms[2] / 3600.0\n\n    if ref in ['S', 'W']:\n        degrees = -degrees\n        minutes = -minutes\n        seconds = -seconds\n\n    return round(degrees + minutes + seconds, 5)\n\ndef get_coordinates(geotags):\n    lat = get_decimal_from_dms(geotags['GPSLatitude'], geotags['GPSLatitudeRef'])\n\n    lon = get_decimal_from_dms(geotags['GPSLongitude'], geotags['GPSLongitudeRef'])\n\n    return (lon, lat)\n\n\n\ndef get_geotagging(exif):\n    if not exif:\n        raise ValueError(\"No EXIF metadata found\")\n\n    geotagging = {}\n    for (idx, tag) in TAGS.items():\n        if tag == 'GPSInfo':\n            if idx not in exif:\n                raise ValueError(\"No EXIF geotagging found\")\n\n            for (key, val) in GPSTAGS.items():\n                if key in exif[idx]:\n                    geotagging[val] = exif[idx][key]\n\n    return geotagging\n\n#_______________________________________________________________________________________________________________________________________#\n\nIn views.py, update home_page function to extract meta data and save the image to database.\nfrom django.contrib.gis.geos import Point\n\n@login_required(login_url='login')\ndef home_page(request):\n    if request.method == \"POST\":\n        form = ImageForm(request.POST, request.FILES)\n        img = Image.open(request.FILES.get(\"image\"))\n        if form.is_valid():\n            try:\n                obj = form.save(commit=False)\n                obj.user = request.user\n                obj.image_url = obj.image.url\n                geotags = get_geotagging(img._getexif())\n                obj.geom = Point(\n                    get_coordinates(geotags)\n                )  # X is longitude, Y is latitude, Point(X,Y)\n                obj.save()\n                messages.success(request, f\"image uploaded succesfully\")\n            except ValueError as e:\n                messages.warning(request, e)\n        else:\n            messages.warning(request, f\"Invalid image type\")\n        return redirect(\"home\")\n    else:\n        Imageform = ImageForm()\n        return render(\n            request, \"GisMap/home.html\", {\"Title\": \"Home Page\", \"image_form\": ImageForm}\n        )"
  },
  {
    "objectID": "posts/2021-django-image-upload/index.html#upload-to-s3-bucket",
    "href": "posts/2021-django-image-upload/index.html#upload-to-s3-bucket",
    "title": "How to Upload Multiple Geotagged Images in Django",
    "section": "Upload to S3 bucket",
    "text": "Upload to S3 bucket\nInstall boto3 package and django-storages. Add to installed packages. Additionally, provide Key:Value AWS credentials to access the bucket and change the default file storage to S3.\npip install django-storages\npip install boto3\nin settings.py\nINSTALLED_APPS = [\n    ...\n    'storages',\n]\n\nAWS_ACCESS_KEY_ID = \"\"\nAWS_SECRET_ACCESS_KEY = \"\"\nAWS_STORAGE_BUCKET_NAME = \"\"\n\nAWS_S3_FILE_OVERWRITE = False\nAWS_DEFAULT_ACL = None\n\nDEFAULT_FILE_STORAGE = 'storages.backends.s3boto3.S3Boto3Storage'\n\nAWS_QUERYSTRING_AUTH = False // removes the query string\nNOTE: Make the bucket public to be able to make HTTP request\nProvide policy to make our s3 bucket public. By default, the bucket is private and no read/wrtie access is provided for user from outside the s3 page. There are other ways to access private bucket by either Limiting access to specific IP addresses or Restricting access to a specific HTTP referer. For simplicity we make the bucket public.\n{\n  \"Version\":\"2012-10-17\",\n  \"Statement\":[\n    {\n      \"Sid\":\"PublicRead\",\n      \"Effect\":\"Allow\",\n      \"Principal\": \"*\",\n      \"Action\":[\"s3:GetObject\",\"s3:GetObjectVersion\"],\n      \"Resource\":[\"arn:aws:s3:::DOC-EXAMPLE-BUCKET/*\"]\n    }\n  ]\n}"
  },
  {
    "objectID": "posts/2021-django-image-upload/index.html#accept-non-geotagged-images",
    "href": "posts/2021-django-image-upload/index.html#accept-non-geotagged-images",
    "title": "How to Upload Multiple Geotagged Images in Django",
    "section": "Accept non-geotagged images",
    "text": "Accept non-geotagged images\nAt this point, we should be able to upload geotagged images to s3 bucket. Non-geotagged images are not yet accepted by the model and thus we create seperate model for it.\nAdditional resource\nWe now make separate model for accepting non-geotagged images similar to ImageUpload model but without PointField.\nclass Photos(models.Model):\n\n    user = models.ForeignKey(User, null=True, on_delete=models.CASCADE)\n    image = models.ImageField(upload_to='photos/',null=True,blank=False)\n    date_created = models.DateTimeField(auto_now_add=True, null=True)\n    image_url = models.URLField(max_length=250, null=True, blank=False)\n\n    class Meta:\n        verbose_name = 'Photo'\n        verbose_name_plural = 'Photos'\n\n    def __str__(self):\n        return self.user.username + \" uploaded image \"+ self.image.name\nIn views.py file, extend the home_page function to add a fallback for non-geotagged images.\nif request.method == \"POST\":\n\n    # images will be in request.FILES\n    post_request, files_request = request.POST, request.FILES\n\n    form = PhotoForm(post_request or None, files_request or None)\n    files = request.FILES.getlist(\n        \"images\"\n    )  # returns files: [&lt;InMemoryUploadedFile: Image_name.jpg (image/jpeg)&gt;, &lt;InMemoryUploadedFile: Image_name.jpg (image/jpeg)&gt;]\n    if form.is_valid():\n        user = request.user\n        for f in files:\n\n            # returns &lt;PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=480x360 at 0x1ED0CCC6280&gt;\n            img = Image.open(f)  \n            \n            try:\n                geotags = get_geotagging(img._getexif())\n                geoimage = ImageUpload(user=user, image=f)\n                geoimageimg_upload.image_url = geoimage.image.url\n                # X is longitude, Y is latitude, Point(X,Y) ; returns eg SRID=4326;POINT (11.88454 43.46708)\n                geoimage.geom = Point(get_coordinates(geotags))\n                geoimage.save()\n            except:\n                nongeoimage = Photos(user=user, image=f)\n                nongeoimage.image_url = nongeoimage.image.url\n                nongeoimage.save()\n    else:\n        print(\"Form invalid\")\n    return redirect(\"home\")\nelse:\n    Imageform = PhotoForm()\n    return render(\n        request, \"GisMap/home.html\", {\"Title\": \"Home Page\", \"image_form\": ImageForm}\n    )"
  },
  {
    "objectID": "posts/2021-django-image-upload/index.html#accept-multiple-images",
    "href": "posts/2021-django-image-upload/index.html#accept-multiple-images",
    "title": "How to Upload Multiple Geotagged Images in Django",
    "section": "Accept multiple images",
    "text": "Accept multiple images\nMake a new form which accepts multiple image files to be uploaded at once.\nclass PhotoForm(forms.ModelForm):\n    images = forms.FileField(widget=forms.ClearableFileInput(attrs={'multiple': True}))\n\n    class Meta:\n        model = Photos\n        fields = ('images',)\nIn home.html, add multiple attribute to allow for multiple selection of images at once.\n                &lt;div class=\"form-group\"&gt;\n                &lt;label for=\"note-image\"&gt;&lt;/label&gt;\n                &lt;input type=\"file\" name=\"images\" class=\"form-control-file\" id=\"note-image\" multiple&gt;\n                &lt;/div&gt;"
  },
  {
    "objectID": "posts/2021-django-image-upload/index.html#final-note",
    "href": "posts/2021-django-image-upload/index.html#final-note",
    "title": "How to Upload Multiple Geotagged Images in Django",
    "section": "Final Note:",
    "text": "Final Note:\nAt this point, you should be able to upload multiple Images to the AWS S3 bucket and have coordinates extracted the geo-tagged images and segregate non-geotagged images.\nYou learnt-\n\nHow to Setup GeoDjango\nHow to Setup AWS S3 bucket\nHow to Extract meta data from Image and store in database using PointField\n\n\nThese steps will ensure you have multiple images uploaded at once and all the geolocation information can be stored in database, which later can be import to QGIS for data visualisation. Although both postgresql and django admin allows users to visualise the data."
  },
  {
    "objectID": "posts/2021-12-22-vector-tiles-and-docker-using-pg-tilerserv/index.html",
    "href": "posts/2021-12-22-vector-tiles-and-docker-using-pg-tilerserv/index.html",
    "title": "Vector tiles and Docker using pg_tilerserv",
    "section": "",
    "text": "In this blog we look at how to serve your geospatial data as vector tiles using pg_tileserv in a docker container."
  },
  {
    "objectID": "posts/2021-12-22-vector-tiles-and-docker-using-pg-tilerserv/index.html#what-are-vector-tiles",
    "href": "posts/2021-12-22-vector-tiles-and-docker-using-pg-tilerserv/index.html#what-are-vector-tiles",
    "title": "Vector tiles and Docker using pg_tilerserv",
    "section": "What are vector tiles?",
    "text": "What are vector tiles?\nVector Tiles are similar to raster tiles, but instead of serving images, vector tiles serve geospatial data which are vectors themselves and not images. This allows for reduced data transfer over a network, faster loading while allowing client side rendering. Moreover, vector tiles allow for flexible styling of your geospatial data since it renders on the client side. All this is not possible with raster tiles and hence vector tiles have gained traction in the last few years.\nOne of the most popular specifications to serve vector tiles is mapbox vector tiles, utilized by many open source tile servers.\nBecause PostGIS can create mapbox vector tiles from vector data, it becomes easy to serve them over the web. Many tileservers use the power of this postGIS functionality to serve vector tiles over the web.\nAs for a visual understanding as to what is different between vector and raster tiles, the following image illustrates that. The red bounding box is the response to clients request to serve vector tiles. Notice the format is pbf as opposed to png for raster tiles."
  },
  {
    "objectID": "posts/2021-12-22-vector-tiles-and-docker-using-pg-tilerserv/index.html#why-use-docker-for-this",
    "href": "posts/2021-12-22-vector-tiles-and-docker-using-pg-tilerserv/index.html#why-use-docker-for-this",
    "title": "Vector tiles and Docker using pg_tilerserv",
    "section": "Why use docker for this?",
    "text": "Why use docker for this?\nUsing docker would expedite the process of starting and “actually” using the applications. It is like sharing your machine with others so that they do not have to install anything to get started. For this reason, it makes complete sense to use docker for moderate to high complexity projects."
  },
  {
    "objectID": "posts/2021-12-22-vector-tiles-and-docker-using-pg-tilerserv/index.html#what-is-pg_tileserve",
    "href": "posts/2021-12-22-vector-tiles-and-docker-using-pg-tilerserv/index.html#what-is-pg_tileserve",
    "title": "Vector tiles and Docker using pg_tilerserv",
    "section": "What is pg_tileserve?",
    "text": "What is pg_tileserve?\nTo create vector tiles, and serve them on the web, you need a middleware that can talk to the database and also serve them on the web. Since pg_tileserve uses a postgis function under the hood, it becomes a default choice to add a lightweight service to serve vector tiles. pg_tileserv returns Mapbox Vector tiles on input of vector geometry. In addition to reading tables from the database, it can handle complex functions to meet our needs.\nST_asMVT, an aggregate function which is used under the hood for pg_tileserv, returns mapbox vector tile format based on google protobuf. While there are other formats such as MBtiles which is sqlite based binary file (can be opened in sqlite), Mapbox Vector Tile format seems to be winning this race and is thus the most popular format currently.\n\nTo get started with serving your vector data to the web using pg_tileserv, we follow the below mentioned steps\n\nDownload pg_tileserv folder from down-git website and save it to your local directory. \n\nThe folder contains all the files required to start a docker container and serve vector tiles.\n└───data/  — would contain all your vector data\n└───load-data.sh — shell script to load data into PostgreSQL\n└───pg_tileserv.env — database URL to connect\n└───docker-compose.yml — \n└───pg.env — environment variable for database\n└───cleanup.sh — assemble multiple containers\n└───README — guide to setup docker by Just van den Broecke\n\nNext, Modify docker-compose.yml file under build-&gt;context to point to the docker file https://github.com/CrunchyData/pg_tileserv.git. Since we did not clone the repository, we specify the Dockerfile using the git link.\n\n\n\nDump all your geospatial data into data dir. This directory will be mounted to the container, once we start it.\nChange the pg_tileserv.env environment file as you wish, to specify the name and password of your database.\n\nNotes on env files: - pg_tilerserv.env file contains the database url which is of the format postgres://your-username:your-password@localhost:5432/your-database-name while pg.env contains credentials for postgres database.\nNotes on docker-compose file - We are mounting data dir from our local system to the work dir in the docker container. - We are mapping port 7800 from our local machine to 7800 to the pg_tileserv container.\nStart Docker Desktop and run docker-compose build in the command line. It will download the image needed from the dockerfile specified. It only downloads the latest alpine image and all other dependencies are installed in the build.\nOnce the database setup is done, we now load data into the database by running either load-data.sh shell script (or) the following command,\n#Load data using shp2pgsql \ndocker-compose exec pg_tileserv_db sh -c \"shp2pgsql -D -s 4326 /work/ne_50m_admin_0_countries.shp | psql -U tileserv -d tileserv\"\nThe above command opens a terminal inside the pg_tileserv_db container and runs the shp2pgsql command.\nWe can use ogr2ogr command line tool if your data is anything other than shapefile. Read this blog by Kat Batuigas to know how to do it.\nFinally, run docker-compose up to start the service. You’d see both containers starting up and your web app being served on port 7800. If you do not see this, stop the container and run again.\n\nOn running the web app in the browser we see our tables visible under Table Layers and the schema it belongs to. We added a few additional layers (public.hydrants and a function layer following steps from README.md) to play around with it."
  },
  {
    "objectID": "posts/2021-12-22-vector-tiles-and-docker-using-pg-tilerserv/index.html#endnote",
    "href": "posts/2021-12-22-vector-tiles-and-docker-using-pg-tilerserv/index.html#endnote",
    "title": "Vector tiles and Docker using pg_tilerserv",
    "section": "Endnote",
    "text": "Endnote\nWe looked at serving vector data as tiles using pg_tileserv and docker container. Docker enables reproducibility and expedites the process of running a web app. Although there are numerous open-source tile servers available, each has its use case and would require testing them out to identify the best tileserver for your use case. You can read a long list of tileservers here.\nSo next time you think to serve large vector data on the web app, make sure to use vector tiles built inside a docker container. It will surely simplify things!\nSource: 1. CrunchyData/pg_tileserv: A very thin PostGIS-only tile server in Go. Takes in HTTP tile requests, executes SQL, returns MVT tiles. (https://github.com/CrunchyData/pg_tileserv/)\n\nLightweight PostGIS Web Services Using pg tileserv and pg featureserv (https://www.youtube.com/watch?v=TXPtocZWr78&t=1s&ab_channel=CrunchyData)\nReference | Vector tiles | Mapbox (https://docs.mapbox.com/vector-tiles/reference/)\nVector Tiles – Geoinformation HSR (https://giswiki.hsr.ch/Vector_Tiles)"
  },
  {
    "objectID": "posts/2021-09-30-polygonize-raster-and-compute-zonal-stats-in-python/index.html",
    "href": "posts/2021-09-30-polygonize-raster-and-compute-zonal-stats-in-python/index.html",
    "title": "Polygonize Raster and Compute Zonal-Statistics in Python",
    "section": "",
    "text": "The output of a clustering algorithm is a raster. But when you want to compute statistics of the clustered raster, it needs to be polygonized.\nA simple way to perform this action is using the gdal command line gdal_polygonize.py script. This script requires the output file format, input raster file and output name of the vector file. You can additionally mask pixel values which you don’t want to convert to polygons. For this example, we would consider a single band image.\npython gdal_polygonize.py raster_file -f \"ESRI Shapefile\" vector_file.shp  layername atrributefieldname\n--nomask allows to include nodata values in the shapefile\natrributefieldname should always be preceded with layername else it would result in an error.\nThe output would result in a vector layer. The number of output polygons is equal to the number of non-NA values. Each neighbouring cell (pixel) which is connected in the raster having the same value is combined to form a single polygon.\nFor instance, consider this 4 x 4 raster. When converted to vector, it resulted in 6 polygons. Note that disconnected similar values form an independent polygon. Each polygon will have an attribute as its pixel value from the raster, in the data type of the image. These would end up being a pair of (polygon, value) for each feature found in the image.\n\n\n\nFig.1 -Converting Raster to Vector using GDAL. The output polygon has attribute associated with its raster value \n\n\nAnother way to polygonize raster programmatically is to use the rasterio library. Since rasterio utilizes GDAL under the hood, it also performs similar action and results in a pair of geometry and raster value. We create a tuple of dictionaries to store each feature output.\n# code to polygonize using rasterio\nfrom rasterio import features\n\n# read the raster and polygonize\nwith rasterio.open(cluster_image_path) as src:\n    image = src.read(1, out_dtype='uint16') \n    #Make a mask!\n    mask = image != 0\n# `results` contains a tuple. With each element in the tuple representing a dictionary containing the feature (polygon) and its associated raster value\nresults = ( {'properties': {'cluster_id': int(v)}, 'geometry': s} \n            for (s, v) in (features.shapes(image, mask=mask, transform=src.transform)))\nOnce we have the raster polygonized, we can use rasterstats library to calculate zonal statistics. We use this library since there is no in-built functionality for rasterio to calculate it.\nThis library has a function zonal_stats which takes in a vector layer and a raster to calculate the zonal statistics. Read more here\nThe parameters to the function are:\n\nvectors: path to an vector source or geo-like python objects\nraster: ndarray or path to a GDAL raster source\n\nand various other options which can be found here\nTo create a vector layer from the tuple results, we use geopandas. There are other libraries (such as fiona) which can also create vector geometry from shapely objects.\nFor raster, we pass the .tif file directly to zonal_stats. The final code looks like the following\nfrom rasterstats import zonal_stats\n\nin_shp = gpd.GeoDataFrame.from_features(results).set_crs(crs=src.crs)\n\n# stats parameter takes in various statistics that needs to be computed \nstatistics= zonal_stats(in_shp,image,stats='min, max, mean, median',\n                geojson_out=True, nodata = -999)\nThe output is a geojson generator when geojson_out is True. we can convert the geojson to dataframe and export as csv for further processing.\nThis way, with the help of geopandas, rasterstats and rasterio, we polygonize the raster and calculate zonal statistics."
  },
  {
    "objectID": "posts/2021-09-08-smap-time-series/index.html",
    "href": "posts/2021-09-08-smap-time-series/index.html",
    "title": "SMAP Time Series",
    "section": "",
    "text": "Farmers in parts of India still rely on groundwater for irrigation. For them to help understand the present condition of their farm, NASA’s Soil Moisture Active Passive (SMAP) satellite data could fill a significant void.\nThe mission collects the kind of local data agricultural and water managers worldwide need.\nThe main output of this data set is surface soil moisture (SSM)(representing approximately the top 5 cm of the soil column on average, given in cm3 /cm3 ) presented on the global 36 km EASE-Grid 2.0. While there are other measurements, we are only restricting ourselves to SSM\nThe SSM product has three main levels. L1, L2, and the latest being L3. SMAP uses a radiometer to detect microwave signals and process to obtain soil moisture. It initially had radar onboard but failed in 2015. Although the product is primarily available in 36 km resolution, with the help of Sentinel-1 Radar product, we now have access to 9 km resolution daily global product as well post 2016.\nWe are going to work with a 36 km product since a time-series can be computationally intensive to download.\nOne such product is L3_SM_P, a daily global product, which is an abbreviation of L3 soil moisture 36 km resolution.\nWe choose Bengaluru as our area of interest and perform the following three steps in sequence\n\nDownload the SMAP L3 data for the latest one month ( August 2021 here).\nExtraction of the soil moisture values from SMAP L3 data over Lat, Lon of Bengaluru in python.\nPlot the time series plot for the extracted soil moisture values for the latest one month.\n\nTo download the SMAP L3 data, we head over to https://nsidc.org/data/SPL3SMP/versions/7 and select a time-period ( in our case for the entire month of August 2021) under the download tab. We then click on the download script button as a python file.\n\n\n\nFig.1 -Downloading python script for the month of August from NSIDC\n\n\nAs can be seen in the picture, we have a download size of 980 mb once we run the python script. The next step would be to download the actual files and extract soil moisture value for the selected lat long. One thing to note is, since the product has a resolution of 36 km and that the entire pixel represents one value, we have to couple together a set of pixels around Bengaluru since the entire region does not overlay in one pixel size.\nWe would be using colaboratory in this entire process since it allows for smooth use of the command line within the notebook itself.\n\n\nWe move the downloaded files to data directory and delete any associated files that comes along with it.\n# Download data: Enter credentials for earth data\n%run /content/download_SPL3.py\n \n# move files to data dir\n!mkdir -p data/L3_SM_P\n!mv *h5 data/L3_SM_P\n!rm *.h5*\n \nNext, get the lat long for EASE grid 2.0. Since we have to locate Bengaluru (study area) and SMAP uses a specific grid system, we download these files.\n!wget https://github.com/nsidc/smap_python_notebooks/raw/main/notebooks/EASE2_M36km.lats.964x406x1.double\n!wget https://github.com/nsidc/smap_python_notebooks/raw/main/notebooks/EASE2_M36km.lons.964x406x1.double\n\n\n\nWe define a python class since there are two half-orbit passes (ascending and descending pass) and we could later combine them easily.\nWe create a read_SML3P method which reads the hdf5 files using the h5py library as an array and removes noisy elements as defined by the user guide. The filename contains the date of acquisition and we extract that.\nWe next define the generate_time_series method to subset the array to our area of interest (Bengaluru) while also taking the mean since there might be more than 1 pixel intersecting the AOI and then return a dataframe with date and the value of Soil Moisture.\nThere are some additional method we define to run and initialise the class which can be read from here\nclass SML3PSoilMoist:\n  \"\"\"\n  get soil moisture from L3 SMAP SCA-V algo for the specified date\n  Parameters\n  ----------\n  soil_moisture: numpy.array\n  flag_id:  [str] Quality flag of retrieved soil moisture using SCA-V\n  var_id: [str] can be replaced with scva algorithm which is the default (baseline)\n  group_id: [str] retrive soil moisture for Ascending or descending pass\n  file_list: [list] of downloaded files; File path of a SMAP L3 HDF5 file\n  -------\n  Returns Soil moisture values and time period as a DataFrame\n  \"\"\"\n \n  def __init__(self, file_list : 'list', orbit_pass: 'str'):\n    \n    ...\n \n  def run_(self):\n    \"\"\"read files and return 3d array and time\"\"\"\n    ...\n \n \n  def read_SML3P(self, filepath):\n    ''' This function extracts soil moisture from SMAP L3 P HDF5 file.\n    # refer to https://nsidc.org/support/faq/how-do-i-interpret-surface-and-quality-flag-information-level-2-and-3-passive-soil\n\n    '''    \n    with h5py.File(filepath, 'r') as f:\n \n        group_id = self.group_id \n        flag_id = self.flag_id\n        var_id = self.var_id\n \n        flag = f[group_id][flag_id][:,:]\n \n        soil_moisture = f[group_id][var_id][:,:]        \n        soil_moisture[soil_moisture==-9999.0]=np.nan;\n        soil_moisture[(flag&gt;&gt;0)&1==1]=np.nan # set to nan expect for 0 and even bits\n \n        filename = os.path.basename(filepath)\n        \n        yyyymmdd= filename.split('_')[4]\n        yyyy = int(yyyymmdd[0:4]); mm = int(yyyymmdd[4:6]); dd = int(yyyymmdd[6:8])\n        date=dt.datetime(yyyy,mm,dd)\n \n    return soil_moisture, date\n \n  def generate_time_series(self, bbox: 'list -&gt; [N_lat, S_lat, W_lon, E_lon]'):\n    \n    N_lat, S_lat, W_lon, E_lon = bbox\n    subset = (lats&lt;N_lat)&(lats&gt;S_lat)&(lons&gt;W_lon)&(lons&lt;E_lon)\n    sm_time = np.empty([self.time_period]);\n    \n    sm_data_3d, times = self.run_()\n    for i in np.arange(0,self.time_period):\n        sm_2d = sm_data_3d[:,:,i]\n        \n        sm_time[i] = np.nanmean(sm_2d[subset]);\n \n    return pd.DataFrame({'time' : times, self.orbit_pass: sm_time })\nLastly, we plot the dataframe using pandas method plot and the result is to be shown to the world.\n\nThis blog helps demonstrates use of SMAP product to generate time series for an entire month of August. You can read more about the specification of the product here\n\nData courtesy: O’Neill et al. doi: https://doi.org/10.5067/HH4SZ2PXSP6A. [31st August, 2021]."
  },
  {
    "objectID": "posts/2021-09-08-smap-time-series/index.html#overview",
    "href": "posts/2021-09-08-smap-time-series/index.html#overview",
    "title": "SMAP Time Series",
    "section": "",
    "text": "Farmers in parts of India still rely on groundwater for irrigation. For them to help understand the present condition of their farm, NASA’s Soil Moisture Active Passive (SMAP) satellite data could fill a significant void.\nThe mission collects the kind of local data agricultural and water managers worldwide need.\nThe main output of this data set is surface soil moisture (SSM)(representing approximately the top 5 cm of the soil column on average, given in cm3 /cm3 ) presented on the global 36 km EASE-Grid 2.0. While there are other measurements, we are only restricting ourselves to SSM\nThe SSM product has three main levels. L1, L2, and the latest being L3. SMAP uses a radiometer to detect microwave signals and process to obtain soil moisture. It initially had radar onboard but failed in 2015. Although the product is primarily available in 36 km resolution, with the help of Sentinel-1 Radar product, we now have access to 9 km resolution daily global product as well post 2016.\nWe are going to work with a 36 km product since a time-series can be computationally intensive to download.\nOne such product is L3_SM_P, a daily global product, which is an abbreviation of L3 soil moisture 36 km resolution.\nWe choose Bengaluru as our area of interest and perform the following three steps in sequence\n\nDownload the SMAP L3 data for the latest one month ( August 2021 here).\nExtraction of the soil moisture values from SMAP L3 data over Lat, Lon of Bengaluru in python.\nPlot the time series plot for the extracted soil moisture values for the latest one month.\n\nTo download the SMAP L3 data, we head over to https://nsidc.org/data/SPL3SMP/versions/7 and select a time-period ( in our case for the entire month of August 2021) under the download tab. We then click on the download script button as a python file.\n\n\n\nFig.1 -Downloading python script for the month of August from NSIDC\n\n\nAs can be seen in the picture, we have a download size of 980 mb once we run the python script. The next step would be to download the actual files and extract soil moisture value for the selected lat long. One thing to note is, since the product has a resolution of 36 km and that the entire pixel represents one value, we have to couple together a set of pixels around Bengaluru since the entire region does not overlay in one pixel size.\nWe would be using colaboratory in this entire process since it allows for smooth use of the command line within the notebook itself.\n\n\nWe move the downloaded files to data directory and delete any associated files that comes along with it.\n# Download data: Enter credentials for earth data\n%run /content/download_SPL3.py\n \n# move files to data dir\n!mkdir -p data/L3_SM_P\n!mv *h5 data/L3_SM_P\n!rm *.h5*\n \nNext, get the lat long for EASE grid 2.0. Since we have to locate Bengaluru (study area) and SMAP uses a specific grid system, we download these files.\n!wget https://github.com/nsidc/smap_python_notebooks/raw/main/notebooks/EASE2_M36km.lats.964x406x1.double\n!wget https://github.com/nsidc/smap_python_notebooks/raw/main/notebooks/EASE2_M36km.lons.964x406x1.double\n\n\n\nWe define a python class since there are two half-orbit passes (ascending and descending pass) and we could later combine them easily.\nWe create a read_SML3P method which reads the hdf5 files using the h5py library as an array and removes noisy elements as defined by the user guide. The filename contains the date of acquisition and we extract that.\nWe next define the generate_time_series method to subset the array to our area of interest (Bengaluru) while also taking the mean since there might be more than 1 pixel intersecting the AOI and then return a dataframe with date and the value of Soil Moisture.\nThere are some additional method we define to run and initialise the class which can be read from here\nclass SML3PSoilMoist:\n  \"\"\"\n  get soil moisture from L3 SMAP SCA-V algo for the specified date\n  Parameters\n  ----------\n  soil_moisture: numpy.array\n  flag_id:  [str] Quality flag of retrieved soil moisture using SCA-V\n  var_id: [str] can be replaced with scva algorithm which is the default (baseline)\n  group_id: [str] retrive soil moisture for Ascending or descending pass\n  file_list: [list] of downloaded files; File path of a SMAP L3 HDF5 file\n  -------\n  Returns Soil moisture values and time period as a DataFrame\n  \"\"\"\n \n  def __init__(self, file_list : 'list', orbit_pass: 'str'):\n    \n    ...\n \n  def run_(self):\n    \"\"\"read files and return 3d array and time\"\"\"\n    ...\n \n \n  def read_SML3P(self, filepath):\n    ''' This function extracts soil moisture from SMAP L3 P HDF5 file.\n    # refer to https://nsidc.org/support/faq/how-do-i-interpret-surface-and-quality-flag-information-level-2-and-3-passive-soil\n\n    '''    \n    with h5py.File(filepath, 'r') as f:\n \n        group_id = self.group_id \n        flag_id = self.flag_id\n        var_id = self.var_id\n \n        flag = f[group_id][flag_id][:,:]\n \n        soil_moisture = f[group_id][var_id][:,:]        \n        soil_moisture[soil_moisture==-9999.0]=np.nan;\n        soil_moisture[(flag&gt;&gt;0)&1==1]=np.nan # set to nan expect for 0 and even bits\n \n        filename = os.path.basename(filepath)\n        \n        yyyymmdd= filename.split('_')[4]\n        yyyy = int(yyyymmdd[0:4]); mm = int(yyyymmdd[4:6]); dd = int(yyyymmdd[6:8])\n        date=dt.datetime(yyyy,mm,dd)\n \n    return soil_moisture, date\n \n  def generate_time_series(self, bbox: 'list -&gt; [N_lat, S_lat, W_lon, E_lon]'):\n    \n    N_lat, S_lat, W_lon, E_lon = bbox\n    subset = (lats&lt;N_lat)&(lats&gt;S_lat)&(lons&gt;W_lon)&(lons&lt;E_lon)\n    sm_time = np.empty([self.time_period]);\n    \n    sm_data_3d, times = self.run_()\n    for i in np.arange(0,self.time_period):\n        sm_2d = sm_data_3d[:,:,i]\n        \n        sm_time[i] = np.nanmean(sm_2d[subset]);\n \n    return pd.DataFrame({'time' : times, self.orbit_pass: sm_time })\nLastly, we plot the dataframe using pandas method plot and the result is to be shown to the world.\n\nThis blog helps demonstrates use of SMAP product to generate time series for an entire month of August. You can read more about the specification of the product here\n\nData courtesy: O’Neill et al. doi: https://doi.org/10.5067/HH4SZ2PXSP6A. [31st August, 2021]."
  },
  {
    "objectID": "posts/2021-07-16-geoserver-query-builder/index.html",
    "href": "posts/2021-07-16-geoserver-query-builder/index.html",
    "title": "Query Geoserver Layer using openlayers",
    "section": "",
    "text": "This blog demonstrates how to display and query all geoserver layers or from a workspace using geoserver REST API. CQL (Common Query Language) filter provided by geoserver is used to query the layer.\nWe create a full stack application, setting up the backend using django and the frontend using vanilla js. The application will later be deployed on aws ec2 instance."
  },
  {
    "objectID": "posts/2021-07-16-geoserver-query-builder/index.html#overview",
    "href": "posts/2021-07-16-geoserver-query-builder/index.html#overview",
    "title": "Query Geoserver Layer using openlayers",
    "section": "",
    "text": "This blog demonstrates how to display and query all geoserver layers or from a workspace using geoserver REST API. CQL (Common Query Language) filter provided by geoserver is used to query the layer.\nWe create a full stack application, setting up the backend using django and the frontend using vanilla js. The application will later be deployed on aws ec2 instance."
  },
  {
    "objectID": "posts/2021-07-16-geoserver-query-builder/index.html#setting-up-the-backend-django",
    "href": "posts/2021-07-16-geoserver-query-builder/index.html#setting-up-the-backend-django",
    "title": "Query Geoserver Layer using openlayers",
    "section": "Setting up the backend (Django)",
    "text": "Setting up the backend (Django)\n\nCreate virtual environment and activate it\nconda create --name djangoEnv\nconda activate djangoEnv\n\n\nStart a new project and create app\ndjango-admin startproject DOGP\npython manage.py startapp gisapp\n\n\nSetup the database\nWe set up postgresql for this exercise. Create a new database and add a postgis extension from it. For more info on how to set up the extension, click here.\nOnce the database is set up on the localhost server, we make changes to the settings.py module in our application.\n# change database\n\nDATABASES = {\n    'default': {\n         'ENGINE': 'django.contrib.gis.db.backends.postgis',\n         'NAME': 'DOGP', # our new database name\n         'USER': 'postgres',\n        'PASSWORD': '1234',\n        'HOST': '127.0.0.1',\n        'PORT': '5432',\n    },\n}\n\n\nAdd installed apps\n\nINSTALLED_APPS = [\n    'gisapp.apps.GisappConfig',\n    'django.contrib.gis',\n]\n\nThere are other setups such as setting up login page and authentication, creating media url root and setting up the url which we are not going to deal with in this blog post.\nOnce the setup is done, we run migrations to reflect those changes in the admin page.\nOn running python manage.py runserver you should see this page.\n\n\n\nFig.1 -Page indicating successful installation of Django\n\n\n\nOur focus will be on the frontend, but the full code can be accessed from here.\nFor querying and displaying layers from geoserver, we first need geoserver installed and running. For more info on how to do that can be found here.\nIn the following steps we setup our basemap layer to be ESRI World Imagery and define an empty vector layer to store the result of query.\n// map setup\nvar maplayer =  new ol.layer.Tile({\n    source: new ol.source.XYZ({\n      attributions: ['Powered by Esri','Source: Esri, DigitalGlobe, GeoEye, Earthstar Geographics, CNES/Airbus DS, USDA, USGS, AeroGRID, IGN, and the GIS User Community'],\n      attributionsCollapsible: false,\n      url: 'https://services.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n      maxZoom: 23\n    }),\n    zIndex: 0\n  })\n\nvar view = new ol.View({\n    projection: 'EPSG:4326',\n    center: [-103.32989447589996, 44.18118547387081],\n    zoom: 7,\n  });\n  \nvar map = new ol.Map({\n    layers: [ maplayer],\n    target: 'map',\n    view: view,\n  });\n\n// define empty vector layer to store query result later\nvar SearchvectorLayerSource =  new ol.source.Vector({\n      \n    })\nvar SearchvectorLayer = new ol.layer.Vector({\n    source:SearchvectorLayerSource\n  });\n  map.addLayer(SearchvectorLayer);\n\n// define headers for authentication and login\nMyHeaders = {'Content-Type': 'application/json', 'Access-Control-Allow-Credentials' : true,\n                'Access-Control-Allow-Origin':'*',\n                'Accept': 'application/json',\n                'Authorization': 'Basic ' + btoa('admin:geoserver')}\nTo access all layers from a particular workspace, the api end point to do that is as follows\n// https://docs.geoserver.org/latest/en/api/#1.0.0/layers.yaml\n/workspaces/{workspaceName}/layers\nTo see this in action, we display all layers from the sf workspace, provided in geoserver by default.\n\n\n\nFig.2 -Geoserver layers from sf workspace\n\n\n\nvar layerList = []; // array to store all the layer info\nvar loginInfo = [\"admin\", \"geoserver\"]; // username and password for geoserver\nvar geoserverURL = geoserver_ip + \":\" + geoserver_port  \n\n// make ajax call to access the sf layer\n$.ajax({\n    url: geoserverURL + '/geoserver/rest/workspaces/sf/layers/',\n    type: 'GET',\n    dataType: 'json',\n    contentType: \"application/json\",\n    beforeSend: function(xhr) {\n         xhr.setRequestHeader (\"Authorization\", \"Basic \" + btoa(loginInfo[0] + \":\" + loginInfo[1]));\n    },\n    success: function(data){\n        for (var i = 0; i &lt; data.layers.layer.length; i++) {\n            layerList.push([data.layers.layer[i].name, data.layers.layer[i].href]);\n        }\n\n    },\n    async: false\n});\nThe output of this ajax call returns us a layerList array containing all the layer name and the url associated with it of size (:, 2)\nThis layer can then be displayed on the frontend by looping over the array and inserting into the div element.\n\n\n\nFig.3 -The layers of workspace sf displayed on the map with some styles applied to it \n\n\n\nThe next step after displaying all the layers of the workspace is to load the features of the layer on selecting a particular layer.\nWhen the layer is ticked we send a request to geoserver to load the features of that layer and add to the map. If the layer is then unticked, we do the opposite and remove the layer from map.\n  function toggleLayer(input) {\n      if (input.checked) {\n          wmsLayer = new ol.layer.Image({\n            source: new ol.source.ImageWMS({\n              url: geoserver_ip+ ':'+geoserver_port + \"/geoserver/wms\",\n              imageLoadFunction: tileLoader,\n              params: { LAYERS: input.value },\n              serverType: \"geoserver\",\n            }),\n            name: input.value,\n          });\n\n        map.addLayer(wmsLayer);\n                    \n      } else {\n          map.getLayers().forEach(layer =&gt; {\n              if (layer.get('name') == input.value) {\n                 map.removeLayer(layer);\n             }\n         });\n      }\n  }\n\n\n\nFig.4 -Displaying layer on map\n\n\n\n\nQuery layer\nWe start with the querying the layer by their attributes. We load all the attributes (as columns) and display as dropdown. We use wfs service and DescribeFeatureType request to load the attributes.\n  function loadprops(layername) {\n      selectedLayer = layername;\n      fetch(\n        geoserver_ip+ ':'+geoserver_port+\"/geoserver/wfs?service=wfs&version=2.0.0&request=DescribeFeatureType&typeNames=\" + \n          layername +\n          \"&outputFormat=application/json\",\n        {\n          method: \"GET\",\n          headers: MyHeaders,\n        }\n      )\n        .then(function (response) {\n          return response.json();\n        })\n        .then(function (json) {\n            var allprops = json.featureTypes[0].properties;\n          var ColumnnamesSelect = document.getElementById(\"Columnnames\");\n              ColumnnamesSelect.innerHTML = ''\n            for (i = 0; i &lt; allprops.length; i++){\n                if (allprops[i].name != 'the_geom') {\n                    ColumnnamesSelect.innerHTML +=\n                      '&lt;option value=\"' +\n                      allprops[i].name +\n                      '\"&gt; ' +\n                      allprops[i].name +\n                      \"&lt;/option&gt;\";\n                }\n  \n            }\n        });\n  }\nUpto this point we have the layer and its features we want to search for. To query the layer we make a fetch call to ows service protocol and pass in the values of feature and the layer we want to query for.\nCQL_filter = column_name + \" = '\" + query_value + \"'\";\n  query_url =geoserver_ip+ ':'+geoserver_port + \"/geoserver/sf/ows?service=WFS&version=1.0.0&request=GetFeature&typeName=\" + selectedLayer +    \"&CQL_FILTER=\" +    CQL_filter +  \"&outputFormat=application%2Fjson\";\n            \n  fetch_search_call(query_url).catch((error) =&gt; {\n  CQL_filter = column_name + \"%20\" + \"ILIKE\" + \"%20%27%25\" + query_value + \"%25%27\";\n    });\nWe define a fetch_search_call function which makes a request to ows service and returns a geojson. We can parse the geojson and display it on the map.\n  \nfunction fetch_search_call(query_url){\n\n    fetch_result = fetch(query_url, {\n        method: \"GET\",\n        headers: MyHeaders,\n      })\n        .then(function (response) {\n          return response.json();\n        })\n        .then(function (json) {\n        \n                SearchvectorLayerSource.clear()\n                SearchvectorLayerSource.addFeatures(\n              new ol.format.GeoJSON({\n              }).readFeatures(json)\n              );\n              if(json.features.length!=0){\n              $('#searchModal').modal('toggle');\n              }\n\n            SearchvectorLayer.set('name','search_polygon_layer')\n            map.getView().fit(SearchvectorLayerSource.getExtent(),  { duration: 1590, size: map.getSize(), padding: [10, 10, 13, 15], maxZoom:16});\n            \n        return fetch_result\n  }\nThe above function queries a feature and adds it to the map as a new layer. If the search is successful, we are zoomed into that location and only the feature queried gets displayed. If the fetch call could not find the match it returns an error which is caught by catch and displays the error to the client.\n\n\n\nFig.5 -Displaying Queried layer by attribute value\n\n\nThis completes the blog on how to query layer and display on the map. Visit the github page to find the working application."
  },
  {
    "objectID": "posts/2021-05-24-polygon-selection-and-area-calculating-in-openlayers/index.html",
    "href": "posts/2021-05-24-polygon-selection-and-area-calculating-in-openlayers/index.html",
    "title": "Polygon Selection and Area Calculation in Openlayers",
    "section": "",
    "text": "In this small demo-blog we look into how to make polygon selections on the map and calculate the area of that polygon on-the-fly. We use openlayers v6 for client side and geoserver to save our vector layers for this exercise.\nI assume readers to have familiarity with setting up geoserver and basics of openlayers."
  },
  {
    "objectID": "posts/2021-05-24-polygon-selection-and-area-calculating-in-openlayers/index.html#overview",
    "href": "posts/2021-05-24-polygon-selection-and-area-calculating-in-openlayers/index.html#overview",
    "title": "Polygon Selection and Area Calculation in Openlayers",
    "section": "",
    "text": "In this small demo-blog we look into how to make polygon selections on the map and calculate the area of that polygon on-the-fly. We use openlayers v6 for client side and geoserver to save our vector layers for this exercise.\nI assume readers to have familiarity with setting up geoserver and basics of openlayers."
  },
  {
    "objectID": "posts/2021-05-24-polygon-selection-and-area-calculating-in-openlayers/index.html#step-1-setup-openlayers",
    "href": "posts/2021-05-24-polygon-selection-and-area-calculating-in-openlayers/index.html#step-1-setup-openlayers",
    "title": "Polygon Selection and Area Calculation in Openlayers",
    "section": "Step 1: Setup openlayers",
    "text": "Step 1: Setup openlayers\nOpenlayers requires you to add these cdns to add their functionality into our application. ### link necessary cdns\n\n &lt;link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/gh/openlayers/openlayers.github.io@master/en/v6.5.0/css/ol.css\" type=\"text/css\"&gt;\n &lt;script src=\"https://cdn.jsdelivr.net/gh/openlayers/openlayers.github.io@master/en/v6.5.0/build/ol.js\"&gt;&lt;/script&gt;\n \nWe are using openlayers to render the request response. since the output of the WFS request is json, we create a new layer with vector source and format as geojson. The strategy:ol.loadingstrategy.bbox tells openlayers to only load features within the bbox. Simply put, if we move to different location, only features within that bbox will appear.\n\n// setup geoserver port\nvar geoserver_ip = 'http://120.0.0.1'\nvar geoserver_port = '8080'\n\n// define vector source\nvar myFlSource = new ol.source.Vector({\n    format: new ol.format.GeoJSON(),\n        url: function (extent){\n            return ( geoserver_ip +':' + geoserver_port + '/geoserver/dronnav/ows?service=WFS&version=1.1.0&request=GetFeature&typeName=dronnav%3Aflorida_bp&maxFeatures=10000&outputFormat=application/json&srsname=EPSG:4326&' + 'bbox=' + extent.join(',') + ',EPSG:4326' );\n        },\n        strategy:ol.loadingstrategy.bbox,\n    });\nWe perform WFS request from geoserver to get our layer florida_bp in this case. The parameters are as explained as follows\n\nservice=WFS : web feature service to perform interaction\ntypename=workspace:florida_bp : specify the workspace and layer name\nversion=1.1.0 : version number\nmaxFeatures = 10000 : since WFS request is computationaly expensive, we restrict to only load 10000 features.\nrequest=GetFeature : request type. There are several other which can be found here\noutputFormat=application/json : the output format as response\nsrsname=EPSG:4326 : coordinate reference system to display on the map\nbbox= : bounding box\n\n// define vector layer\nvar floridaLayer = new ol.layer.Vector({\n    source: myFlSource,\n    style: new ol.style.Style({\n        fill: new ol.style.Fill({\n            color: 'rgba(1, 1, 255, .2)',\n            }),\n        stroke: new ol.style.Stroke({\n            color: 'rgba(1, 1, 255, .5)',\n            width: 2,\n        }),\n        }),\n        minZoom: 16, // this will allows us to send request only when the zoom is atleast 16\n});\nOnce the layer is defined, we need to add this layer to the map. We can either use map.addLayer(layername) or add to array in the map (Fig.1)\n// add ESRI basemap\nvar project_maplayer =    new ol.layer.Tile({\n    source: new ol.source.XYZ({\n        attributions: ['Powered by Esri',\n                                        'Source: Esri, DigitalGlobe, GeoEye, Earthstar Geographics, CNES/Airbus DS, USDA, USGS, AeroGRID, IGN, and the GIS User Community'],\n        attributionsCollapsible: false,\n        url: 'https://services.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n        maxZoom: 23\n    }),\n    zIndex: 0\n});\n\n// add view with projection set to EPSG 4326 for the map\nvar project_view = new ol.View({\n    projection: 'EPSG:4326',\n    center: [-81.80808208706726, 27.285095000261222],\n    zoom: 7,\n});\n\n// define the map with all the layers created previously\nvar Projectmap = new ol.Map({\n    layers: [project_maplayer, floridaLayer],\n    overlays: [overlay],\n    target: 'project_map', // the div element `id` in html page\n    view: project_view,\n});\n\n\n\nFig.1 -The Map layer with building footprints (floridaLayer) added to the map with the style we specified for each feature"
  },
  {
    "objectID": "posts/2021-05-24-polygon-selection-and-area-calculating-in-openlayers/index.html#get-feature-info-on-click",
    "href": "posts/2021-05-24-polygon-selection-and-area-calculating-in-openlayers/index.html#get-feature-info-on-click",
    "title": "Polygon Selection and Area Calculation in Openlayers",
    "section": "Get feature info on click",
    "text": "Get feature info on click\nAfter adding basemap and our layer to the map served via geoserver, we are now ready to get information on-click. We use forEachFeatureAtPixel method on our layer to send a WFS request to our geoserver and recive a response in json format. We change the style of the building on click (Fig.2). The area is calculated using formatArea function which utilises ol.sphere.getArea and transform method to calculate area and change CRS.\n\n  /*  select ploygon the feature and get area and store the features */\n  var selected = []; // contains all features\n  var selected_area = []; // contains area of feature, one-to-one\n  \n  Projectmap.on('singleclick', function (e) {\n  Projectmap.forEachFeatureAtPixel(e.pixel, function (f, l) {\n    var mycoordinate = e.coordinate\n    storef = f  // feature\n    /* if click is on polygon, then select the feature */\n  if ( f.getGeometry()   instanceof  ol.geom.MultiPolygon ) {\n          \n        var selIndex = selected.indexOf(f);\n            // console.log(selIndex)\n        if (selIndex &lt; 0) {\n            selected.push(f);\n            selected_area.push( formatArea(f) ); // formatArea function returns the area in ft2\n            f.setStyle(highlightStyle); // change style on click\n        } else {\n            selected.splice(selIndex, 1);\n            selected_area.splice( selIndex, 1);\n            f.setStyle(undefined);\n        }\n     }\n\n      })\n\n      /* update the tags with no of selected feature and total area combined */\n      document.getElementById('status-selected').innerHTML = '&nbsp;' + selected.length + ' selected features';\n      document.getElementById('status-selected_area').innerHTML = '&nbsp;' + selected_area.reduce(getSum, 0) + ' ft&lt;sup&gt;2&lt;/sup&gt;';\n      \n    });\n    \n\n    \n  /* style for selected feature on click  */\n  var highlightStyle = new ol.style.Style({\n    fill: new ol.style.Fill({\n    color: '#f0b88b',\n    }),\n    stroke: new ol.style.Stroke({\n    color: '#f0b88b',\n    width: 3,\n    }),\n  });\n\n\n  /*  function for calculating area of the polygon (feature) selected */\n  function formatArea (polygon){\n   var area = ol.sphere.getArea(polygon.getGeometry().transform('EPSG:4326', 'EPSG:3857')); // transform to projected coordinate system.\n   var output;\n   output = Math.round(area * 100*10.7639) / 100  ; //in ft2\n   polygon.getGeometry().transform('EPSG:3857', 'EPSG:4326' ) //convert back to geographic crc\n   return output;\n  }\n  \n  /*  function for array sum */\n  function getSum(total, num) {\n  return total + Math.round(num);\n  }\n\n\n\nFig.2 -The floridaLayer building footprints selected with the style we specified for each feature"
  },
  {
    "objectID": "posts/2021-05-24-polygon-selection-and-area-calculating-in-openlayers/index.html#final-comments",
    "href": "posts/2021-05-24-polygon-selection-and-area-calculating-in-openlayers/index.html#final-comments",
    "title": "Polygon Selection and Area Calculation in Openlayers",
    "section": "Final comments",
    "text": "Final comments\nThis post demonstrates the use of strategy:ol.loadingstrategy.bbox to load only the features that cover the bounding box. We use this strategy since WFS service is resouce intensive and our server cannot handle millions of HTTP request at once.\nWe also see the use of forEachFeatureAtPixel method to select our building footprints. On click of the feature we change the style using setStyle method.\nAdditionally, we saw how to change projection on-the-fly using ol.sphere.getArea method. A word of caution while using EPSG:3857. My AOI was on the equator and thus calculating area does not result in significant error. But if the AOI is in temperate zone then adopt suitable projection CRS.\nLayer Credit: Microsoft buidling footprints https://github.com/microsoft/USBuildingFootprints"
  },
  {
    "objectID": "events/index.html",
    "href": "events/index.html",
    "title": "Events",
    "section": "",
    "text": "Had a workshop session for NITK on python data handling and mining. You can find the slides here. The associated github code is here"
  },
  {
    "objectID": "events/index.html#june",
    "href": "events/index.html#june",
    "title": "Events",
    "section": "",
    "text": "Visited varkala for the first time. An absolute delight to be at south/north cliff with an amazing company."
  },
  {
    "objectID": "events/index.html#may-2025",
    "href": "events/index.html#may-2025",
    "title": "Events",
    "section": "May 2025",
    "text": "May 2025\nGot rid of the old phone - it was very slow and couldn’t help but throw it away."
  },
  {
    "objectID": "events/index.html#april-2025",
    "href": "events/index.html#april-2025",
    "title": "Events",
    "section": "April 2025",
    "text": "April 2025\nWe concluded 20th Let’s Talk Spatial event - and it happened at IIIT Bangalore. With Professor Jasmeet Judge and Dr. Subhash as speakers. It went wonderfully well - and good discussion as well"
  },
  {
    "objectID": "events/index.html#march-2025",
    "href": "events/index.html#march-2025",
    "title": "Events",
    "section": "March 2025",
    "text": "March 2025\nWe had our 19th meetup at Varaha. It was good to be back organising meetups! And it was really good :) You can check out the video here\nConducted a hands-on workshop on Earth engine for climate at BNMIT\nWas part of India Open LandCover Network meet that happened in collaboration with MapBiomas. Thoughtful discussion and lessons on how India can productionize this effort."
  },
  {
    "objectID": "events/index.html#febuary-2025",
    "href": "events/index.html#febuary-2025",
    "title": "Events",
    "section": "Febuary 2025",
    "text": "Febuary 2025\nBack to work. A lot of progress made thus far, but a long way to go. Let’s Talk Spatial meetup has been in hibernation for some time and we are starting it next month. Got to dive deep into workings of LLMs and how they could be used for geospatial technology. Very promising front to explore next couple of months."
  },
  {
    "objectID": "events/index.html#january-2025",
    "href": "events/index.html#january-2025",
    "title": "Events",
    "section": "January 2025",
    "text": "January 2025\nI got married to love of my life. The whole month passed in a jiffy. We went to maldives at end of the month. Wonderful experience!"
  },
  {
    "objectID": "events/index.html#december-2024",
    "href": "events/index.html#december-2024",
    "title": "Events",
    "section": "December 2024",
    "text": "December 2024\nWedding preparation is in beast MODE. Too less of a time remaining.\nI got to deliver a talk on GeoPython at NMIT. As IEEE GRSS Young Professional Chair of Bangalore Section, I got to engage with the NMIT chapter and help them understand why this is the best time to be in Geospatial Industry"
  },
  {
    "objectID": "events/index.html#november-2024",
    "href": "events/index.html#november-2024",
    "title": "Events",
    "section": "November 2024",
    "text": "November 2024\nI got promoted at my job. Now I am a senior geospatial scientist ;)\nGot done with wedding photoshoot, phew!"
  },
  {
    "objectID": "events/index.html#october-2024",
    "href": "events/index.html#october-2024",
    "title": "Events",
    "section": "October 2024",
    "text": "October 2024\nStarted search for the best wedding outfits! Now I know a lot about clothing - and have a better sense of judgement!"
  },
  {
    "objectID": "events/index.html#september-2024",
    "href": "events/index.html#september-2024",
    "title": "Events",
    "section": "September 2024",
    "text": "September 2024\nAttended SAC event on the occasion of National Space Day! The excitement was palpable, as it was the first time private industry was invited inside SAC.\nFriends (and colleagues) went on a vacation to Puducherry! Maybe the last one before my marriage."
  },
  {
    "objectID": "events/index.html#august-2024",
    "href": "events/index.html#august-2024",
    "title": "Events",
    "section": "August 2024",
    "text": "August 2024\nConducted a workshop on QGIS for Let’s Talk Spatial. You can watch it here"
  },
  {
    "objectID": "events/index.html#july-2024",
    "href": "events/index.html#july-2024",
    "title": "Events",
    "section": "July 2024",
    "text": "July 2024\nMet with an accident on two-wheeler while driving back home! Broke my leg, WFH for atleast a week!\nConducted (online) workshop on QGIS for ACIWRM. First time an online workshop - was super helpful for me to revise all the important concepts. The team loved it! Email me at amanbagrecha.blr@gmail.com if you’d like to conduct such workshops!\nGot Selected for IADF School happening in Italy! Fingers crossed - hopefully my visa gets accepted!"
  },
  {
    "objectID": "events/index.html#june-2024",
    "href": "events/index.html#june-2024",
    "title": "Events",
    "section": "June 2024",
    "text": "June 2024\nBirthday month - and had to visit a lot of event planners for my wedding! It is tiring TBH :("
  },
  {
    "objectID": "events/index.html#may-2024",
    "href": "events/index.html#may-2024",
    "title": "Events",
    "section": "May 2024",
    "text": "May 2024\nConducted workshop at NIE on Google Earth Engine for the 2nd time. The students loved it, again!\nLet’s Talk Spatial had its 16th meetup at Microsoft Reactor Space, Bangalore! on geospatial AI. The recording can be watched here"
  },
  {
    "objectID": "events/index.html#april-2024",
    "href": "events/index.html#april-2024",
    "title": "Events",
    "section": "April 2024",
    "text": "April 2024\nI found my life partner in my best friend and travel buddy - I am engaged!\nHosted a hackathon in collaboration with SkyServe. A good number of participants from across the country took the challenge to create AI models for the Edge! Details here. The webinar can be found here"
  },
  {
    "objectID": "events/index.html#march-2024",
    "href": "events/index.html#march-2024",
    "title": "Events",
    "section": "March 2024",
    "text": "March 2024\nTook a small vacation with Family to beat the Bangalore heat, off to Goa!\nHosted a meetup in collobration with Proto. The event revolved around maps and content. Super fun audience and speakers. The live stream can be watched here"
  },
  {
    "objectID": "events/index.html#febuary-2024",
    "href": "events/index.html#febuary-2024",
    "title": "Events",
    "section": "Febuary 2024",
    "text": "Febuary 2024\nI conducted a workshop on Google Earth Engine at The National Institute of Engineering, Mysore. Also attended AGM of (IEEE GRSS Bangalore Section](https://site.ieee.org/bangalore-grss/) at Manipal Institute of Management, MAHE, Bangalore."
  },
  {
    "objectID": "events/index.html#january-2024",
    "href": "events/index.html#january-2024",
    "title": "Events",
    "section": "January 2024",
    "text": "January 2024\nIt was a hectic month both professionally and personally! So many things piled up!"
  },
  {
    "objectID": "events/index.html#december-2023",
    "href": "events/index.html#december-2023",
    "title": "Events",
    "section": "December 2023",
    "text": "December 2023\nAttended (and organised) InGARSS 2023, which happened in IIIT Bangalore. I was the YP Chair - helping organize 2 events.\nI took year end break and went on vacation. This year was by far the most I’ve travelled. Wish to do the same this year (maybe even more)!"
  },
  {
    "objectID": "events/index.html#november-2023",
    "href": "events/index.html#november-2023",
    "title": "Events",
    "section": "November 2023",
    "text": "November 2023\nAttended FOSS4G Asia in Seoul - co-conducted workshop on GeoPython and also gave a talk on Sentinel-1 Image Analysis Flood detection using cloud native tooling. South Korea was a experience to be lived. It was chilly outside, but I had good company throughout. Found Indian food luckily :)\nWe also had our 13th meetup at Indian Institute of Human Settlements in collaboration with OpenStreetMap Bengaluru - to talk about OSM and iD editor. The presenters also showcased map making using QGIS.\nOur paper Climate responsive design for road surface drainage systems: a case study for city of Bengaluru got accepted in Urban water Journal!"
  },
  {
    "objectID": "events/index.html#october-2023",
    "href": "events/index.html#october-2023",
    "title": "Events",
    "section": "October 2023",
    "text": "October 2023\nI conducted workshop on Planetary Computer and STAC API which was also the 12th meetup conducted by Let’s Talk Spatial.\nI attended Geosmart Conference in Hyderabad. A lot of geospatial companies were present. Made many new connections and got to know about latest trends and how the indian geospatial community is up to."
  },
  {
    "objectID": "events/index.html#september-2023",
    "href": "events/index.html#september-2023",
    "title": "Events",
    "section": "September 2023",
    "text": "September 2023\nI joined GalaxEye!\nWe organised mapping party in Chamrajpet, Bengaluru! Read my OSM Diary here."
  },
  {
    "objectID": "events/index.html#june---july-2023",
    "href": "events/index.html#june---july-2023",
    "title": "Events",
    "section": "June - July 2023",
    "text": "June - July 2023\nI attended FOSS4G for the very first time and gave two talks. I had the most memorable experience in Prizren. Read in-depth blog here."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "disclaimer/index.html",
    "href": "disclaimer/index.html",
    "title": "Aman Bagrecha",
    "section": "",
    "text": "Opinions expressed on this Site are the authors’ own in his personal capacity. They do not reflect the views of the any organisation or company he is associated with."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aman Bagrecha",
    "section": "",
    "text": "I am a geospatial scientist specializing in GIS, AI/ML, and Remote Sensing. I advocate and contribute to open-source software - Xarray, QGIS, STAC, GeoPython stack to name a few. Beyond my professional life, I enjoy jogging and playing badminton.\nI graduated from Rashtreeya Vidyalaya College of Engineering in Civil Engineering where I was the president of ASCE Student Chapter, leading the team to Concrete Canoe Competition among various other activities. During college, I was also a Research Intern at Indian Institute of Science working on non-stationary rainfall intensity surface drainage design.\nI co-founded Let’s Talk Spatial, a community for geospatial enthusiasts, hosting events and collaborating with other communities like OpenStreetMap Bengaluru and IEEE GRSS Bangalore chapter. I actively contribute to open-source projects and share my knowledge through blogs, videos, and workshops.\nAs the Young Professional Chair of IEEE GRSS Bangalore chapter, I co-organize various events.\nI’ve spoken at opensource conferences like FOSS4G Kosovo and FOSS4G Asia Seoul, sharing my insights on open-source technologies.\nMy publications can be found on Google Scholar.\n\n\n\nGalaxEye Space | Geospatial Scientist | Sept 2023 – present\nBlue Sky Analytics | Data Scientist | May 2022 – Sept 2023\nSatyukt Analytics | Remote Sensing Engineer | Sept 2021 – Mar 2022\nRotten Grapes Pvt. Ltd | Web GIS Developer | Feb 2021 – May 2021"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Aman Bagrecha",
    "section": "",
    "text": "I am a geospatial scientist specializing in GIS, AI/ML, and Remote Sensing. I advocate and contribute to open-source software - Xarray, QGIS, STAC, GeoPython stack to name a few. Beyond my professional life, I enjoy jogging and playing badminton.\nI graduated from Rashtreeya Vidyalaya College of Engineering in Civil Engineering where I was the president of ASCE Student Chapter, leading the team to Concrete Canoe Competition among various other activities. During college, I was also a Research Intern at Indian Institute of Science working on non-stationary rainfall intensity surface drainage design.\nI co-founded Let’s Talk Spatial, a community for geospatial enthusiasts, hosting events and collaborating with other communities like OpenStreetMap Bengaluru and IEEE GRSS Bangalore chapter. I actively contribute to open-source projects and share my knowledge through blogs, videos, and workshops.\nAs the Young Professional Chair of IEEE GRSS Bangalore chapter, I co-organize various events.\nI’ve spoken at opensource conferences like FOSS4G Kosovo and FOSS4G Asia Seoul, sharing my insights on open-source technologies.\nMy publications can be found on Google Scholar."
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Aman Bagrecha",
    "section": "",
    "text": "GalaxEye Space | Geospatial Scientist | Sept 2023 – present\nBlue Sky Analytics | Data Scientist | May 2022 – Sept 2023\nSatyukt Analytics | Remote Sensing Engineer | Sept 2021 – Mar 2022\nRotten Grapes Pvt. Ltd | Web GIS Developer | Feb 2021 – May 2021"
  },
  {
    "objectID": "posts/2021-05-24-pdf-and-email-creation/index.html",
    "href": "posts/2021-05-24-pdf-and-email-creation/index.html",
    "title": "Django rest framework PDF creation and email via gmail SMTP and reportLab",
    "section": "",
    "text": "Ever wanted to send email with attachements that too in django? And have the attachments created from the user input? This post tries to solve exactly that."
  },
  {
    "objectID": "posts/2021-05-24-pdf-and-email-creation/index.html#overview",
    "href": "posts/2021-05-24-pdf-and-email-creation/index.html#overview",
    "title": "Django rest framework PDF creation and email via gmail SMTP and reportLab",
    "section": "",
    "text": "Ever wanted to send email with attachements that too in django? And have the attachments created from the user input? This post tries to solve exactly that."
  },
  {
    "objectID": "posts/2021-05-24-pdf-and-email-creation/index.html#main-steps",
    "href": "posts/2021-05-24-pdf-and-email-creation/index.html#main-steps",
    "title": "Django rest framework PDF creation and email via gmail SMTP and reportLab",
    "section": "Main steps",
    "text": "Main steps\nIn this blog we create PDF using Report Lab and email it to the user using gmail SMTP service. All actions are performed in Django."
  },
  {
    "objectID": "posts/2021-05-24-pdf-and-email-creation/index.html#step-1-create-django-view-to-serialize-data",
    "href": "posts/2021-05-24-pdf-and-email-creation/index.html#step-1-create-django-view-to-serialize-data",
    "title": "Django rest framework PDF creation and email via gmail SMTP and reportLab",
    "section": "Step 1 : create django view to serialize data",
    "text": "Step 1 : create django view to serialize data\nTo begin with, we create a view CreatePDF which accepts POST request and the data gets passed onto CreatePDFSerializer which serializes our data and validates it. If our data is valid, we generate PDF using generate_pdf function and email to the recipent (emailaddress of the users) using the sendPDF function. If everything does not execute properly, we return an error response else a success.\nThe local variable myresponse is a dictionary which helps us manage the response for each return statement in the correct format as expected by response method.\n\nSUCCESS = 'success'\nERROR = 'error'\nmessage_list = ['response', 'status', 'message'] # eg: [\"success\", 201, \"successfully upload the file\"]\n\n@csrf_exempt\n@api_view(['POST',])\ndef CreatePDF(request):\n    myresponse = {k: [] for k in message_list}\n\n    try:\n        myData = request.data\n        # serialier the data\n        serializer = serializers.CreatePDFSerializer(data=myData)  \n        if serializer.is_valid():\n            try:\n                sendPDF(**myData.dict())  # create pdf and send email\n            except Exception as e:\n                RequestResponse(\n                    myresponse,\n                    ERROR,\n                    status.HTTP_400_BAD_REQUEST,\n                    {\"Email\": [\"Could not send mail!\"]},\n                )\n                return Response(data=myresponse)\n            \n            account = serializer.save()\n            RequestResponse(\n                myresponse,\n                SUCCESS,\n                status.HTTP_201_CREATED,\n                {\"Success\": [f\"Inspection Report e-mailed to {account.EmailAddress}!\"]},\n            )\n            return Response(data=myresponse)\n\n        RequestResponse(\n            myresponse, ERROR, status.HTTP_400_BAD_REQUEST, serializer.errors\n        )\n        return Response(data=myresponse)\n    \n    except Exception as e:\n        print(e)\n        RequestResponse(\n            myresponse,\n            ERROR,\n            status.HTTP_500_INTERNAL_SERVER_ERROR,\n            {\"Error\": [\"Internal Server Error\"]},\n        )\n        return Response(data=myresponse)"
  },
  {
    "objectID": "posts/2021-05-24-pdf-and-email-creation/index.html#step-2-generate-pdf-using-report-lab",
    "href": "posts/2021-05-24-pdf-and-email-creation/index.html#step-2-generate-pdf-using-report-lab",
    "title": "Django rest framework PDF creation and email via gmail SMTP and reportLab",
    "section": "step 2: Generate PDF using Report Lab",
    "text": "step 2: Generate PDF using Report Lab\nIn views.py we create a function to generate pdf using Report Lab package. This allows us to define the page size and line strings with text placement to be included.\ndef generate_pdf(**Mydata):\n    y = 700\n    buffer = io.BytesIO()  # in memory create pdf\n    p = canvas.Canvas(buffer, pagesize=letter)\n    p.setFont('Helvetica', 14)\n    p.drawString(220, y, Mydata['Title'])\n    p.drawString(450, y, 'Date:' + timezone.now().strftime('%Y-%b-%d'))\n    p.line(30, 675, 550, 675)\n    p.drawString(220, y - 300, 'Time'\n                 + str(Mydata['time']))\n    p.showPage()\n    p.save()\n    pdf = buffer.getvalue()\n    buffer.close()\n    return pdf"
  },
  {
    "objectID": "posts/2021-05-24-pdf-and-email-creation/index.html#step-3-send-email-via-smtp-backend",
    "href": "posts/2021-05-24-pdf-and-email-creation/index.html#step-3-send-email-via-smtp-backend",
    "title": "Django rest framework PDF creation and email via gmail SMTP and reportLab",
    "section": "step 3: Send Email via SMTP backend",
    "text": "step 3: Send Email via SMTP backend\nIn views.py, we create sendPDF function which calls the generate_pdfto generate PDF and attaches the pdf to the email using the EmailMessage class method attach. We additionally need to setup backend for smtp service and host user which is to be done in settings.py.\n# views.py\ndef sendPDF(**Mydata):\n    pdf = generate_pdf(**Mydata)\n    msg = EmailMessage(Mydata['Title'], \" Your Report is ready! \", settings.EMAIL_HOST_USER, to=[Mydata['EmailAddress']])\n    msg.attach(f\"{Mydata['Title']}.pdf\", pdf, 'application/pdf')\n    msg.content_subtype = \"html\"\n    resp = msg.send()\n    print(\"resp:\" , resp)\nIn settings.py\n# settings.py\nEMAIL_BACKEND = 'django.core.mail.backends.smtp.EmailBackend'\nEMAIL_HOST = \"smtp.gmail.com\"\nEMAIL_HOST_USER = 'your_email@gmail.com'\nEMAIL_HOST_PASSWORD = 'your_password'\nEMAIL_PORT = 587\nEMAIL_USE_TLS = True\nAt this point we have been able to successfully setup and send email with attachment."
  },
  {
    "objectID": "posts/2021-06-09-validating-lulc-classes-in-qgis/index.html",
    "href": "posts/2021-06-09-validating-lulc-classes-in-qgis/index.html",
    "title": "Validating LULC classes in QGIS",
    "section": "",
    "text": "Any land-use land cover classification needs to be validated with ground-truth data to measure the accuracy. A key single-valued statistic to determine the effectiveness of classification is Cohen’s kappa. This validation metric has been fairly widely used for unbalanced classification as well which expresses a level of agreement between two annotators on a classification problem.\nThe objective of this quality assessment was to validate the land cover map performed on June, 2020 sentinel-2 imagery by k-means classification algorithm, thus providing a statistical measure of overall class predictions. The validation was done using an independent set of sample points (~500) generated randomly following stratified random sampling design, to capture the variance within the class\nAfter running the tool, the sample points were manually assigned to the ground-truth class. The ground-truth dataset was taken to be Bing-satellite imagery as a proxy for field data. Each sample point was labelled by visual inspection on the ground-truth dataset."
  },
  {
    "objectID": "posts/2021-06-09-validating-lulc-classes-in-qgis/index.html#the-problem-statement",
    "href": "posts/2021-06-09-validating-lulc-classes-in-qgis/index.html#the-problem-statement",
    "title": "Validating LULC classes in QGIS",
    "section": "",
    "text": "Any land-use land cover classification needs to be validated with ground-truth data to measure the accuracy. A key single-valued statistic to determine the effectiveness of classification is Cohen’s kappa. This validation metric has been fairly widely used for unbalanced classification as well which expresses a level of agreement between two annotators on a classification problem.\nThe objective of this quality assessment was to validate the land cover map performed on June, 2020 sentinel-2 imagery by k-means classification algorithm, thus providing a statistical measure of overall class predictions. The validation was done using an independent set of sample points (~500) generated randomly following stratified random sampling design, to capture the variance within the class\nAfter running the tool, the sample points were manually assigned to the ground-truth class. The ground-truth dataset was taken to be Bing-satellite imagery as a proxy for field data. Each sample point was labelled by visual inspection on the ground-truth dataset."
  },
  {
    "objectID": "posts/2021-06-09-validating-lulc-classes-in-qgis/index.html#step-1-classify-image",
    "href": "posts/2021-06-09-validating-lulc-classes-in-qgis/index.html#step-1-classify-image",
    "title": "Validating LULC classes in QGIS",
    "section": "Step 1: Classify Image",
    "text": "Step 1: Classify Image\n\nLoad raster Image\nOpen K-means clustering for grids under SAGA tools. Select the raster Image as grid and in this case we specify 4 classes\n\n\n\n\nFig.1 -K-means clustering on sentinel-2 Image\n\n\n\nClick Run\n\n\nAt this stage we have unsupervised k-means clustering output ready (Fig.2).\n\n\n\n\nFig.2 -classification of RR Nagar, Bengaluru. Classes- Forest, Urban, water, Bareland"
  },
  {
    "objectID": "posts/2021-06-09-validating-lulc-classes-in-qgis/index.html#step-2-convert-to-polygon-vector-format",
    "href": "posts/2021-06-09-validating-lulc-classes-in-qgis/index.html#step-2-convert-to-polygon-vector-format",
    "title": "Validating LULC classes in QGIS",
    "section": "Step 2: Convert to polygon (vector format)",
    "text": "Step 2: Convert to polygon (vector format)\n\nSelect Polygonize (Raster to Vector) tool under GDAL-&gt;Raster Conversion\nSelect the classified image as input. Leave everything else as default. The output would be a Vectorised scratch layer.\n\n\n\n\nFig.3 -Convert Raster to vector\n\n\n\nNote the name of the field (DN here). This will be used later.\n\n\nFix geometries (this step is important here to avoid any error in further steps) Vector Geometry-&gt;Fix Geometry\n\n\n\n\nFig.4 -Fixing topology issues with Fix Geometry Toolbox"
  },
  {
    "objectID": "posts/2021-06-09-validating-lulc-classes-in-qgis/index.html#step-3-dissolve-the-layer-on-dn-field",
    "href": "posts/2021-06-09-validating-lulc-classes-in-qgis/index.html#step-3-dissolve-the-layer-on-dn-field",
    "title": "Validating LULC classes in QGIS",
    "section": "Step 3: Dissolve the layer on DN field",
    "text": "Step 3: Dissolve the layer on DN field\nIn this step we dissolve the layer based on the DN value. This will ensure that each polygon can be evaluated based on the land class type which is needed for stratified random sampling.\n\n\n\nFig.5 -Dissolve toolbox to dissolve polygon on  DN  value\n\n\n\nMake sure to select dissolve field as DN"
  },
  {
    "objectID": "posts/2021-06-09-validating-lulc-classes-in-qgis/index.html#step-4-create-stratified-random-samples",
    "href": "posts/2021-06-09-validating-lulc-classes-in-qgis/index.html#step-4-create-stratified-random-samples",
    "title": "Validating LULC classes in QGIS",
    "section": "Step 4: Create stratified random samples",
    "text": "Step 4: Create stratified random samples\nGo to Vector-&gt;research tools-&gt; Random Points inside Polygon and set Sampling Strategy = Points Density and Point count or density = 0.001.\nNote: The value 0.001 signify 1 point for 1/0.001 m2 of area, given that the units is meters.\n\n\n\nFig.6 - One sample point is generated for each 1000 m2 of area"
  },
  {
    "objectID": "posts/2021-06-09-validating-lulc-classes-in-qgis/index.html#step-5-extract-raster-values-to-sample-layer",
    "href": "posts/2021-06-09-validating-lulc-classes-in-qgis/index.html#step-5-extract-raster-values-to-sample-layer",
    "title": "Validating LULC classes in QGIS",
    "section": "Step 5: Extract raster values to sample layer",
    "text": "Step 5: Extract raster values to sample layer\nWe extract the raster value, which is essentially the land cover class for the classified image. We use Sample Raster Values function here (Fig.7). The input layer is the random points we generated earlier and the the raster layer is the classified image. The output adds a new column to the sample points layer with the prediction class of the land-cover (Fig.8).\n\n\n\nFig.7 -Running Sample Raster Value to extract Raster values for the input points\n\n\n\n\n\nFig.8 -The corresponding Attribute Table with Predicted Class  PREDICTED_1 for each feature"
  },
  {
    "objectID": "posts/2021-06-09-validating-lulc-classes-in-qgis/index.html#step-6-ground-truth-labelling-using-bing-maps",
    "href": "posts/2021-06-09-validating-lulc-classes-in-qgis/index.html#step-6-ground-truth-labelling-using-bing-maps",
    "title": "Validating LULC classes in QGIS",
    "section": "Step 6: Ground Truth Labelling using Bing maps",
    "text": "Step 6: Ground Truth Labelling using Bing maps\nAt this stage we are ready to validate the image using Bing maps as ground truth. We turn on the edit mode and create new field named Actual class. THen we visually inspect the class on the map and note the land-cover class. Once we inspect all the sample points we can use cohens Kappa statistics to determine the validation result. Alternatively, simply calculating the accuracy would also suffice the need."
  },
  {
    "objectID": "posts/2021-06-09-validating-lulc-classes-in-qgis/index.html#step-7-add-other-field-to-the-attribute-table-with-reclassification",
    "href": "posts/2021-06-09-validating-lulc-classes-in-qgis/index.html#step-7-add-other-field-to-the-attribute-table-with-reclassification",
    "title": "Validating LULC classes in QGIS",
    "section": "Step 7: Add other field to the attribute table with reclassification",
    "text": "Step 7: Add other field to the attribute table with reclassification\nWe can use the Field Calculator to generate verbose text for each label in our feature class and display labels for the prediction.\n-- in field calculator to increase verbosity\nCASE WHEN PREDICTED_1 is 2 THEN 'Urban' \nWHEN PREDICTED_1 is 1 THEN 'Bareland'\nWHEN PREDICTED_1 is 4 THEN 'Forest'\nWHEN PREDICTED_1 is 3 THEN 'Urban'\nEND\n\n\n\nFig.9 -Predicted classes (foreground) vs ground truth (background)\n\n\nWith this we come to end of the post. Now, validation accuracy can be reported for k-means classification."
  },
  {
    "objectID": "posts/2021-07-24-contour-maps-in-qgis/index.html",
    "href": "posts/2021-07-24-contour-maps-in-qgis/index.html",
    "title": "Contour Maps in QGIS",
    "section": "",
    "text": "Most of the time, we are equipped with a discrete set of sample points (of temperature, rainfall etc) and are tasked with generating a continuous surface. This is where spatial interpolation comes into picture. The objective is to estimate the most probable value at an unknown location with a set of known points within the extent of sample points.\nMethods to perform spatial interpolation: 1. TIN: Triangular Irregular Network forms contiguous, non-overlapping triangles by dividing the geographic space on set of sample points\n\nIDW: Inverse Distance Weighted interpolation method estimates cell values by weighted average of sample data. The closer the point, the more weight assigned. We can fix the radius of influence or the total sample points to weigh for cell value.\nSpline: Also called french curves. It uses a mathematical function that minimizes overall surface curvature, resulting in a smooth surface that passes through the input points.\nKriging: A group of geostatistical techniques to interpolate the value of a random field at an unobserved location from observations of its value at a nearby location. It is implemented using semi-variogram.\n\nIn this blog, we create surface plots for Rainfall Correction Factors, which is indicative of how much the climate impacts a hydraulic structure based on the return period it is designed for.\nThese RCF are useful for hydraulic structures such as dams, storm water drains, and spillways. These RCF are derived from Global Climate Models (GCMs) which models future scenarios. Not considering these factors can lead to reduced life time of the structure.\nWe calculate the RCF for each point for a grid of lat,lon around the indian subcontinent. These RCF are as a result of intensive computational simulations run in matlab which is out of scope for this blog.\n\n\nOur data is in the csv format with each column of the RP_ family representing the return period the Rainfall Correction Factor is estimated for.\n\n\n\nFig.1 -sample data points with key location and return period of RCF\n\n\nThis file can be imported into qgis from the layers panel and adding a delimited text layer. Once the layer is added, we export as shapefile so as to ease the process of automating the workflow which comes in handy later at the end of the blog.\n\n\n\nFig.2 -Add the csv file using Add delimited text layer\n\n\nWe load the sampled points and add an India boundary as the base vector to later clip the features to our area of Interest.\n\n\n\nFig.3 -Points equally spaced around the Indian state. Each point represent a RCF value\n\n\n\n\n\nFor demonstration let us take an example to run through the entire process of generating surface raster and styling which can be later automated using python in qgis.\nWe use these sampled locations of points to generate a surface using TIN Interpolation readily available as a toolbox in qgis. The input parameter for the vector layer is our shapefile of points while the interpolation attribute is going to be the RP_ family of columns.\n\n\n\nFig.4 -TIN interpolation in QGIS\n\n\nThe output of the interpolation with pixel size of 0.01 is shown below. The extent was set to the boundary of Indian state.\n\n\n\nFig.5 -Output surface raster with 0.01 pixel size\n\n\nWe can go a step further and derive contours using the contour toolbox provided in qgis.\n\n\n\n\n\n\nFig.6 -Generate contours from the surface raster\n\n\n\n\n\nFig.7 -Output as contour lines with 0.1 as interval\n\n\nA better way to get the contour lines is by changing the symbology of the raster to contours and providing an interval. This exact method will be employed later in this post.\n\n\n\nSo far we have looked into creating surface raster for an individual return period. But we have several other return periods and we do not want to repeat ourselves. Thus we write a tiny python code to automate this workflow.\nWe derive the RCFs for return period of 5year, 10year, 25year, 50year\n# specify the output location for saving the files\nOUTPATH = 'D:\\\\gcm_qgis\\\\'\n\n# loop over different return periods from the shapefile\nfor i,j in enumerate(['2y', '10y', '25y', '50y'], 3):\n\n    # specify the shapefile containing the RCP values\n    MYFILE = 'D:\\\\gcm_qgis\\\\RCP_avg.shp|layername=RCP_avg::~::0::~::{}::~::0'.format(i)\n\n    # Run interpolation and do not save the output permanently\n    RESULTS = processing.run(\"qgis:tininterpolation\", \n    {'INTERPOLATION_DATA': MYFILE,\n    'METHOD':1,\n    'EXTENT':'68.205600900,97.395561000,6.755997100,37.084107000 [EPSG:4326]',\n    'PIXEL_SIZE':0.01,\n    'OUTPUT':'TEMPORARY_OUTPUT'})\n\n    # clip the temporary output from prev step and save the files.\n    processing.runAndLoadResults(\"gdal:cliprasterbymasklayer\", \n    {'INPUT':RESULTS['OUTPUT'],\n    'MASK':'C:/Users/91911/Downloads/india-osm.geojson.txt|layername=india-osm.geojson',\n    'SOURCE_CRS':None,'TARGET_CRS':None,'NODATA':None,\n    'ALPHA_BAND':False,\n    'CROP_TO_CUTLINE':True,\n    'KEEP_RESOLUTION':False,'SET_RESOLUTION':False,'X_RESOLUTION':None,\n    'Y_RESOLUTION':None,\n    'MULTITHREADING':False,'OPTIONS':'',\n    'DATA_TYPE':0,\n    'EXTRA':'',\n    'OUTPUT':os.path.join(OUTPATH, 'RCP_avg_' + j + '.tif')})\n    iface.messageBar().pushMessage(\n        'Success:', 'Output file written at ', level=Qgis.Success)\nOur output would save and display the contour files with RCP_avg_{return_period} where return period ranges from [2,5,10,25,50]\nThe code first fetches our shapefile, which is used to 1. create temporary TIN interpolation rasters 3. clipped to india boundary using clip raster by mask layer\nOnce we have the rasters for each return period, we style the raster using singleband pseudocolor in Equal Interval mode ranging from 1.0 - 1.8 in steps of 0.1\nWe make a copy of the raster layer and place it above it, giving it a contour style at an interval of 0.1\nWe copy each return period and set the styling to be of contour as seen in the figure. This allows for a better visual representation of the regions with same the values.\n\n\n\nFig.8 -Styling the copy of surface raster\n\n\nThe final output can be seen in the below figure.\n\n\n\nFig.9 -Final output with contours overlaid on top of surface themself"
  },
  {
    "objectID": "posts/2021-07-24-contour-maps-in-qgis/index.html#overview",
    "href": "posts/2021-07-24-contour-maps-in-qgis/index.html#overview",
    "title": "Contour Maps in QGIS",
    "section": "",
    "text": "Most of the time, we are equipped with a discrete set of sample points (of temperature, rainfall etc) and are tasked with generating a continuous surface. This is where spatial interpolation comes into picture. The objective is to estimate the most probable value at an unknown location with a set of known points within the extent of sample points.\nMethods to perform spatial interpolation: 1. TIN: Triangular Irregular Network forms contiguous, non-overlapping triangles by dividing the geographic space on set of sample points\n\nIDW: Inverse Distance Weighted interpolation method estimates cell values by weighted average of sample data. The closer the point, the more weight assigned. We can fix the radius of influence or the total sample points to weigh for cell value.\nSpline: Also called french curves. It uses a mathematical function that minimizes overall surface curvature, resulting in a smooth surface that passes through the input points.\nKriging: A group of geostatistical techniques to interpolate the value of a random field at an unobserved location from observations of its value at a nearby location. It is implemented using semi-variogram.\n\nIn this blog, we create surface plots for Rainfall Correction Factors, which is indicative of how much the climate impacts a hydraulic structure based on the return period it is designed for.\nThese RCF are useful for hydraulic structures such as dams, storm water drains, and spillways. These RCF are derived from Global Climate Models (GCMs) which models future scenarios. Not considering these factors can lead to reduced life time of the structure.\nWe calculate the RCF for each point for a grid of lat,lon around the indian subcontinent. These RCF are as a result of intensive computational simulations run in matlab which is out of scope for this blog.\n\n\nOur data is in the csv format with each column of the RP_ family representing the return period the Rainfall Correction Factor is estimated for.\n\n\n\nFig.1 -sample data points with key location and return period of RCF\n\n\nThis file can be imported into qgis from the layers panel and adding a delimited text layer. Once the layer is added, we export as shapefile so as to ease the process of automating the workflow which comes in handy later at the end of the blog.\n\n\n\nFig.2 -Add the csv file using Add delimited text layer\n\n\nWe load the sampled points and add an India boundary as the base vector to later clip the features to our area of Interest.\n\n\n\nFig.3 -Points equally spaced around the Indian state. Each point represent a RCF value\n\n\n\n\n\nFor demonstration let us take an example to run through the entire process of generating surface raster and styling which can be later automated using python in qgis.\nWe use these sampled locations of points to generate a surface using TIN Interpolation readily available as a toolbox in qgis. The input parameter for the vector layer is our shapefile of points while the interpolation attribute is going to be the RP_ family of columns.\n\n\n\nFig.4 -TIN interpolation in QGIS\n\n\nThe output of the interpolation with pixel size of 0.01 is shown below. The extent was set to the boundary of Indian state.\n\n\n\nFig.5 -Output surface raster with 0.01 pixel size\n\n\nWe can go a step further and derive contours using the contour toolbox provided in qgis.\n\n\n\n\n\n\nFig.6 -Generate contours from the surface raster\n\n\n\n\n\nFig.7 -Output as contour lines with 0.1 as interval\n\n\nA better way to get the contour lines is by changing the symbology of the raster to contours and providing an interval. This exact method will be employed later in this post.\n\n\n\nSo far we have looked into creating surface raster for an individual return period. But we have several other return periods and we do not want to repeat ourselves. Thus we write a tiny python code to automate this workflow.\nWe derive the RCFs for return period of 5year, 10year, 25year, 50year\n# specify the output location for saving the files\nOUTPATH = 'D:\\\\gcm_qgis\\\\'\n\n# loop over different return periods from the shapefile\nfor i,j in enumerate(['2y', '10y', '25y', '50y'], 3):\n\n    # specify the shapefile containing the RCP values\n    MYFILE = 'D:\\\\gcm_qgis\\\\RCP_avg.shp|layername=RCP_avg::~::0::~::{}::~::0'.format(i)\n\n    # Run interpolation and do not save the output permanently\n    RESULTS = processing.run(\"qgis:tininterpolation\", \n    {'INTERPOLATION_DATA': MYFILE,\n    'METHOD':1,\n    'EXTENT':'68.205600900,97.395561000,6.755997100,37.084107000 [EPSG:4326]',\n    'PIXEL_SIZE':0.01,\n    'OUTPUT':'TEMPORARY_OUTPUT'})\n\n    # clip the temporary output from prev step and save the files.\n    processing.runAndLoadResults(\"gdal:cliprasterbymasklayer\", \n    {'INPUT':RESULTS['OUTPUT'],\n    'MASK':'C:/Users/91911/Downloads/india-osm.geojson.txt|layername=india-osm.geojson',\n    'SOURCE_CRS':None,'TARGET_CRS':None,'NODATA':None,\n    'ALPHA_BAND':False,\n    'CROP_TO_CUTLINE':True,\n    'KEEP_RESOLUTION':False,'SET_RESOLUTION':False,'X_RESOLUTION':None,\n    'Y_RESOLUTION':None,\n    'MULTITHREADING':False,'OPTIONS':'',\n    'DATA_TYPE':0,\n    'EXTRA':'',\n    'OUTPUT':os.path.join(OUTPATH, 'RCP_avg_' + j + '.tif')})\n    iface.messageBar().pushMessage(\n        'Success:', 'Output file written at ', level=Qgis.Success)\nOur output would save and display the contour files with RCP_avg_{return_period} where return period ranges from [2,5,10,25,50]\nThe code first fetches our shapefile, which is used to 1. create temporary TIN interpolation rasters 3. clipped to india boundary using clip raster by mask layer\nOnce we have the rasters for each return period, we style the raster using singleband pseudocolor in Equal Interval mode ranging from 1.0 - 1.8 in steps of 0.1\nWe make a copy of the raster layer and place it above it, giving it a contour style at an interval of 0.1\nWe copy each return period and set the styling to be of contour as seen in the figure. This allows for a better visual representation of the regions with same the values.\n\n\n\nFig.8 -Styling the copy of surface raster\n\n\nThe final output can be seen in the below figure.\n\n\n\nFig.9 -Final output with contours overlaid on top of surface themself"
  },
  {
    "objectID": "posts/2021-07-24-contour-maps-in-qgis/index.html#final-comments",
    "href": "posts/2021-07-24-contour-maps-in-qgis/index.html#final-comments",
    "title": "Contour Maps in QGIS",
    "section": "Final comments",
    "text": "Final comments\nWe looked at various spatial interpolation technique and automated workflow to derive spatially interpolated surface raster.\nSources:\n\nComparison of Spatial Interpolation Techniques Using Visualization and Quantitative Assessment\nSpatial Analysis QGIS"
  },
  {
    "objectID": "posts/2021-09-19-overlay-cropped-raster-with-vector-layer/index.html",
    "href": "posts/2021-09-19-overlay-cropped-raster-with-vector-layer/index.html",
    "title": "Overlay cropped raster with vector layer",
    "section": "",
    "text": "I recently faced a problem of having to plot “cropped raster” layer and a vector layer on the same axes. It is known that we first need to identify the spatial extent of each layer, having the same coordinate reference system.\nRasterio does offer a plotting function show which can plot a raster layer with the correct spatial extent for you when we pass the dataset reader object.\nWhen we pass a reader object, the spatial extent is automatically read by show function.\nMoreover, if we pass a numpy array to the show function, the spatial extent of that array has to be explicitly passed using the transform parameter of the show function since the numpy array does not know the corner location of the raster and thus the plot would begin with x,y: 0,0 as shown below.\nBut what if you want to plot a subset of the raster image, in the sense that you would like to slice the image arbitrarily and plot it. When you slice the image, the affine transformation is not the same anymore and thus plotting the sliced image would result in a plot having the spatial extent of the original image while the sliced image being magnified (Fig. 2).\nTo avert this problem, we need to find the new affine transformation of the cropped image. Luckily rasterio has a window_transform method on the dataset reader which can compute the new transformation from the old one by passing the bounds of the layer. The window_transform function can either take a 2D N-D array indexer in the form of a tuple ((row_start, row_stop), (col_start, col_stop)) or provide offset as written in its documentation"
  },
  {
    "objectID": "posts/2021-09-19-overlay-cropped-raster-with-vector-layer/index.html#cropped-raster-and-vector-overlay",
    "href": "posts/2021-09-19-overlay-cropped-raster-with-vector-layer/index.html#cropped-raster-and-vector-overlay",
    "title": "Overlay cropped raster with vector layer",
    "section": "Cropped raster and vector overlay",
    "text": "Cropped raster and vector overlay\nThe above method returns the new affine transformation, which can be passed to the show function for the numpy array through the transform parameter. We also change the read method instead of slicing the array by window parameter to maintain uniformity\n# load raster\nwith rs.open(path_to_file, \"r\") as src:\n    # window =  (((row_start), (row_stop)), ((col_start), (col_stop)))\n    img = src.read(1, window = ((1,-1), (1,-1)))\n    f, ax = plt.subplots(figsize=(9,9))\n    show(img, transform=src.window_transform(((1,-1), (1,-1))), ax=ax)\n\n    _ = vector_layer.plot(ax=ax)\n\n\n\nFig.3 - Overlay of cropped raster and vector. Notice the updated spatial extent \n\n\nThe show method is helpful for plotting rasters or even RGB images for that matter. One of the differences with matplotlib’s plotting is the order of axes. show expects it the bands to be the last axis while matplotlib, the first. It can also plot 4-band image, which is almost always the for satellite images. While there is an extent paramter in matplotlib’s plotting function, show function is much tidier and straight-forward to implement cropped raster and overlay vector layer on it."
  },
  {
    "objectID": "posts/2021-09-30-three-ways-to-change-projection-of-raw-csv/index.html",
    "href": "posts/2021-09-30-three-ways-to-change-projection-of-raw-csv/index.html",
    "title": "Two ways to Programmatically change projection of raw CSV",
    "section": "",
    "text": "Often, field values are collected in the Geographic Coordinate Reference System as CSV or ASCII so that it can be universally used. But when you want to perform any kind of analysis on these values, there is a need to reproject them into a Projected Coordinate Reference System for the specific area. Although there are many ways that exist now with desktop GIS, these methods can be cumbersome if you have thousands of files to reproject.\nThis task of reprojecting raw CSV can be accomplished using GDAL although it is not straightforward. It requires an indication of geographic data of a CSV file which is provided using VRT (GDAL virtual Raster). More advanced tools now exist which are either built on top of GDAL or are very similar. GeoPandas and pyproj are two such libraries which can help us reproject our raw CSV on-the-fly.\nWe first look at how this task can be accomplished using the GDAL command line."
  },
  {
    "objectID": "posts/2021-09-30-three-ways-to-change-projection-of-raw-csv/index.html#using-geopandas",
    "href": "posts/2021-09-30-three-ways-to-change-projection-of-raw-csv/index.html#using-geopandas",
    "title": "Two ways to Programmatically change projection of raw CSV",
    "section": "Using GeoPandas",
    "text": "Using GeoPandas\nWith its simple and intuitive API, GeoPandas allows us to read, reproject CRS and write files on-the-fly.\nin_path = './'\nout_path = './output'\nfiles= [f for f in os.listdir(in_path) if f.endswith('.csv')]\ninput_crs = 'EPSG:4326'\noutput_crs = 'EPSG:32644'\n\nif not os.path.exists(out_path):\n    os.mkdir(out_path)\n\nfor file in files:\n    df = pd.read_csv(file, header=None)\n    gdf = gpd.GeoDataFrame(\n        df, crs=input_crs , geometry=gpd.points_from_xy(df.iloc[:,0], df.iloc[:,1]))\n\n    gdf.to_crs(output_crs, inplace=True)\n    gdf.iloc[:,0] = gdf.geometry.x # replace x\n    gdf.iloc[:,1] = gdf.geometry.y # replace y\n    \n    # export reprojected csv \n    gdf.iloc[:,:-1].to_csv(os.path.join(out_path, file), index=False )\nIn the above code, we loop through our CSV files. For each file, we create a GeoDataFrame and change the CRS. Lastly, we replace the coordinates with reprojected one.\n\nEndnote\nThere is another way I found by using pyproj library which is quite verbose but performs reprojection on-the-fly. To read about the pyproj method, refer here."
  },
  {
    "objectID": "posts/2021-crud-in-django-rest-framework/index.html#what-will-you-learn",
    "href": "posts/2021-crud-in-django-rest-framework/index.html#what-will-you-learn",
    "title": "Full Fledged CRUD application using DRF and Token Authentication",
    "section": "What will you learn",
    "text": "What will you learn\nToo Long; Didn’t Read \n\n\n\nMarkdown\nLess\n\n\n\n\nDRF\nCreate API end points for CRUD\n\n\nToken Authentication\nAdd security and authorised access\n\n\nFetch API calls\nConsume API from front-end\n\n\nPassword Reset\nSend email to reset your forgotton password"
  },
  {
    "objectID": "posts/2021-crud-in-django-rest-framework/index.html#step-one-basic-django-project-setup",
    "href": "posts/2021-crud-in-django-rest-framework/index.html#step-one-basic-django-project-setup",
    "title": "Full Fledged CRUD application using DRF and Token Authentication",
    "section": "1. Step one : Basic Django Project setup",
    "text": "1. Step one : Basic Django Project setup\nCreate virtual environment\nconda create --name djangoEnv\nActivate the environment\nconda activate djangoEnv\nInstall the dependencies\nconda install django\nNow, in your command line\ncreate project django-admin startproject tutorial\ncreate app python manage.py startapp Accountsapp\ncreate superuser python manage.py createsuperuser\nNow that we have the project and app installed your structure should look like this (insert picture here)\nRegister the app in file as follows\nIn settings.py\nInstalled_apps = [ \n    'Accountsapp.apps.AccountsappConfig',\n    ...\n]\nWe now create our own custom model named MyAccounts\nIn models.py\nfrom django.db import models\nfrom django.contrib.auth.models import AbstractBaseUser, BaseUserManager\n\nfrom django.conf import settings\nfrom django.db.models.signals import post_save\nfrom django.dispatch import receiver\nfrom rest_framework.authtoken.models import Token\n\n\nclass MyAccountManager(BaseUserManager):\n    def create_user(self, email, username, password=None):\n        if not email:\n            raise ValueError('Users must have an email address')\n        if not username:\n            raise ValueError('Users must have a username')\n\n\n        user = self.model(\n            email=self.normalize_email(email),\n            username=username,\n        )\n\n        user.set_password(password)\n        user.save(using=self._db)\n        return user\n\n    def create_superuser(self, email, username, password):\n        user = self.create_user(\n            email=self.normalize_email(email),\n            password=password,\n            username=username,\n            \n        )\n        user.is_admin = True\n        user.is_staff = True\n        user.is_superuser = True\n        user.save(using=self._db)\n        return user\n\n# creating custom model of \"User\" base model. \nclass MyAccount(AbstractBaseUser):\n    email                   = models.EmailField(verbose_name=\"email\", max_length=60, unique=True)\n    username                = models.CharField(max_length=30, unique=True)\n    date_joined             = models.DateTimeField(verbose_name='date joined', auto_now_add=True)\n    last_login              = models.DateTimeField(verbose_name='last login', auto_now=True)\n    is_admin                = models.BooleanField(default=False)\n    is_active               = models.BooleanField(default=True)\n    is_staff                = models.BooleanField(default=False)\n    is_superuser            = models.BooleanField(default=False)\n\n\n    USERNAME_FIELD = 'email'   # username_field is the one which should be unique and will be compared by django for not creating multiple users with same email.\n\n    REQUIRED_FIELDS = ['username'] \n\n    objects = MyAccountManager()\n\n    def __str__(self):\n        return self.email\n\n    # For checking permissions. to keep it simple all admin have ALL permissons\n    def has_perm(self, perm, obj=None):\n        return self.is_admin\n\n    # Does this user have permission to view this app? (ALWAYS YES FOR SIMPLICITY)\n    def has_module_perms(self, app_label):\n        return True\nTo tell django we are overwriting the default user model, we do the following\nIn settings.py\nAUTH_USER_MODEL = Accounts.MyAccounts\nNow we makemigrates to register the model in our database\npython manage.py makemigrations\npython manage.py migrate\nAnd for the model to be visible in admin section we do the following\nIn admin.py\nfrom django.contrib import admin\nfrom .models import MyAccount\n\nadmin.site.register(MyAccount) # Register your models here.\nFor now the our project is setup. We move to Django Rest Framework setup"
  },
  {
    "objectID": "posts/2021-crud-in-django-rest-framework/index.html#setup-django-rest-framework-with-authentication",
    "href": "posts/2021-crud-in-django-rest-framework/index.html#setup-django-rest-framework-with-authentication",
    "title": "Full Fledged CRUD application using DRF and Token Authentication",
    "section": "2. Setup Django Rest Framework with Authentication",
    "text": "2. Setup Django Rest Framework with Authentication\nInstall dependeny\nconda install djangorestframework\nLike any other app, django rest framework is also an app. so we add it to the list of installed apps. We additionally add authtoken app for user authentication which we are shortly going to intergrate in our CRUD application\nIn settings.py\nINSTALLED_APPS = [\n    # my apps\n    'Accountsapp.apps.AccountsappConfig',\n    # restframework\n    'rest_framework',\n    'rest_framework.authtoken',\n    ...\n    \n]\nWe are going to be using Token Authentication in this application. DRF documentation recommends it as the default. Let Us setup the Default authentication class before actually utilising it.\nIn settings.py\nREST_FRAMEWORK = {\n    'DEFAULT_AUTHENTICATION_CLASSES': [\n        'rest_framework.authentication.TokenAuthentication',\n        \n    ]\n}\nThe last thing before we actually start writing code is to perform migration. The rest_framework.authtoken app provides Django database migrations.\nAs done previously on command line\npython manage.py makemigrations\npython manage.py migrate\nWe have completed the logistics for setting up DRF"
  },
  {
    "objectID": "posts/2021-crud-in-django-rest-framework/index.html#building-crud-application",
    "href": "posts/2021-crud-in-django-rest-framework/index.html#building-crud-application",
    "title": "Full Fledged CRUD application using DRF and Token Authentication",
    "section": "3. Building CRUD application",
    "text": "3. Building CRUD application\nWe would first create a folder called api inside our to seperate codebase for API and vanila CRUD\nInside API folder create four files,\n\n__init__.py\nserializers.py\nviews.py\nurls.py\n\nIn serializers.py\nfrom rest_framework import serializers \nfrom Accountsapp.models import MyAccount # import our custom model\n\n\n# provide fields in meta, expression and in MyAccount. for admin page login and edit,  is_admin and is_staff should be true\nclass RegistrationSerializer(serializers.ModelSerializer):\n\n    # additional fields \n    password2 = serializers.CharField(style={'input_type': 'password'}, write_only=True)\n    is_superuser =serializers.BooleanField(write_only=True)\n    \n    class Meta:\n        model = MyAccount\n        # mention the fields you want to display when request is sent. \n        fields = ['id','email', 'username', 'password', 'password2',  'is_superuser']\n        extra_kwargs = {\n                'password': {'write_only': True},  # tells django to not display the password for others to see\n        }   \n\n\n    def save(self):\n\n        account = MyAccount(\n                    email=self.validated_data['email'],\n                    username=self.validated_data['username'],\n                    # is_admin=self.validated_data['is_admin'],\n                    is_superuser= self.validated_data['is_superuser'],\n                )\n        password = self.validated_data['password']\n        password2 = self.validated_data['password2']\n        if password != password2:\n            raise serializers.ValidationError({'password': 'Passwords must match.'})\n        account.set_password(password)\n        account.save()\n        return account\n\n\nclass UpdateSerializer(serializers.ModelSerializer):\n\n    class Meta:\n        model = MyAccount\n        # mention the fields you want to display when request is sent. \n        fields = ['id', 'username', 'email']\n        extra_kwargs = {\n                'password': {'read_only': True},  #  password cannot be edited from here\n        }\n\nNote : Do not try to update the password from serializers. There is another technique which we will deal with in later section.\n\nThe serializers in REST framework work very similarly to Django’s Form and ModelForm classes. The two major serializers that are most popularly used are ModelSerializer and HyperLinkedModelSerialzer.\n\nIn views.py\nfrom rest_framework import status\nfrom rest_framework.response import Response\nfrom rest_framework.permissions import IsAuthenticated, IsAdminUser\nfrom django.contrib.auth import authenticate\nfrom rest_framework.authentication import TokenAuthentication\nfrom rest_framework.decorators import api_view, authentication_classes, permission_classes\n\nfrom . import serializers \nfrom Accountsapp.models import MyAccount\nfrom rest_framework.authtoken.models import Token\n\n# user views\nfrom django.http import JsonResponse\nfrom django.views.decorators.csrf import csrf_exempt\nfrom rest_framework.parsers import JSONParser\nfrom django.core.exceptions import ObjectDoesNotExist\nimport json\n\n# login {built-in django}\nfrom django.contrib.auth import login \nfrom django.contrib.auth.decorators import login_required\n\n\n\n# get all users\n@api_view([\"GET\"])\n@csrf_exempt\n@permission_classes([IsAuthenticated,])\n@authentication_classes([TokenAuthentication])\ndef get_users(request):\n    try:\n        user_profile = MyAccount.objects.all() \n        serializer = serializers.RegistrationSerializer(user_profile, many=True)\n        return Response( {'USER_PROFILE':serializer.data}, status= status.HTTP_200_OK)\n    except ObjectDoesNotExist:\n        return JsonResponse({'Response': 'You do not have authorization to access this page'}, status=status.HTTP_401_UNAUTHORIZED)\n\n\n\n# get given user\n@api_view(['GET'])\n@csrf_exempt\n@permission_classes([IsAuthenticated,])\n@authentication_classes([TokenAuthentication])\ndef get_given_user(request, pk):\n    try:\n        user_profile = MyAccount.objects.get(pk=pk)\n    except ObjectDoesNotExist:\n        return JsonResponse({\"missing\": \"The requested object does not exist\"}, status=status.HTTP_404_NOT_FOUND)\n\n    if request.method == 'GET':  \n        serializer = serializers.RegistrationSerializer(user_profile)\n        token = Token.objects.get(user=user_profile).key\n        return JsonResponse({'given_user_profile': serializer.data, 'token':token})\n   \n\n\n# add user\n@csrf_exempt\n@api_view(['POST'])\ndef user_add_view(request):\n        serializer = serializers.RegistrationSerializer( data=request.data)\n        if serializer.is_valid():\n            account = serializer.save()\n            token, _ = Token.objects.get_or_create(user=account)\n            return Response(serializer.data, status=status.HTTP_201_CREATED,  headers={'Authorization': 'Token ' + token.key})\n        return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)\n\n\n\n# update user\n@api_view([\"PUT\",'GET'])\n@csrf_exempt\n@permission_classes([IsAuthenticated,])\n@authentication_classes([TokenAuthentication])\ndef update_user(request, pk):\n\n    try:\n        user_profile = MyAccount.objects.get(id=pk)\n    except ObjectDoesNotExist:\n        return Response({'response': \"given object does not exist\"}, status=status.HTTP_404_NOT_FOUND)\n\n    user = request.user\n    try:\n        data =  {i:j for i,j in request.query_params.items()}\n        print(data)\n        serializer = serializers.UpdateSerializer(user_profile, data=data)\n        if serializer.is_valid():\n            user= serializer.save()\n            token, _ = Token.objects.get_or_create(user=user)\n            return Response({\"response\": \"success\", 'data' :serializer.data}, status=status.HTTP_201_CREATED,  headers={'Authorization': 'Token ' + token.key})\n        return Response(serializer.errors, status=status.HTTP_400_BAD_REQUEST)\n\n    except ObjectDoesNotExist as e:\n        return JsonResponse({'error': str(e)}, safe=False, status=status.HTTP_404_NOT_FOUND)\n    except Exception:\n        return JsonResponse({'error': 'Something terrible went wrong'}, safe=False, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n\n\n\n# delete user\n@api_view([\"DELETE\",'GET']) \n@csrf_exempt\n@permission_classes([IsAuthenticated])\n@authentication_classes([TokenAuthentication])\ndef delete_user(request, pk):\n\n    try:\n        user_profile = MyAccount.objects.get(id=pk)\n    except ObjectDoesNotExist:\n        return JsonResponse({'response': \"given object does not exist\"}, safe=False, status=status.HTTP_404_NOT_FOUND)\n\n    user = request.user\n    if user_profile != user: \n        return JsonResponse({'response':\"You don't have permission to delete the record.\"}, safe=False, status=status.HTTP_401_UNAUTHORIZED)\n\n    try:\n        user_profile.delete()  #retuns 1 or 0\n        return JsonResponse({'user_delete': \"record deleted\"}, safe=False, status=status.HTTP_200_OK)\n    except ObjectDoesNotExist as e:\n        return JsonResponse({'error': str(e)}, safe=False, status=status.HTTP_404_NOT_FOUND)\n    except Exception:\n        return JsonResponse({'error': 'Something terrible went wrong'}, safe=False, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n\n\n\n# login view and get token\n@api_view([\"POST\", ])\ndef drflogin(request):\n\n    email = request.data.get(\"email\")\n    username = request.data.get(\"username\")\n    password = request.data.get(\"password\")\n    account = MyAccount.objects.filter(email=email) | MyAccount.objects.filter(username=username)\n    if not account:\n        return Response({\"error\": \"Login failed\"}, status=status.HTTP_401_UNAUTHORIZED)\n    # authenticate(email=email, password=password)  # returns none if not authenticated\n    account = authenticate(email=account[0].email, password=password)\n    token, _ = Token.objects.get_or_create(user=account)\n    login(request,account)  \n    renderer= Response({\"response\" : \"Successfully authenticated\",  \"pk\": account.pk, \"username\": account.username, \"token\": token.key }, template_name= \"Accountsapp/loginuser.html\", headers={'Authorization': 'Token ' + token.key})\n    return renderer\nSetup end points for our API\nIn views.py\n\nfrom django.urls import path, include\nfrom . import views as drf_views\n\n\napp_name = 'Accountsapp'\n\nurlpatterns = [\n\n    path('drf_users/', drf_views.get_users, name= 'drf_users'),\n    path('drf_user/&lt;int:pk&gt;/', drf_views.get_given_user, name= 'drf_get_user'),\n    path('drf_updateuser/&lt;int:pk&gt;/', drf_views.update_user, name= 'drf_updateusers'),\n    path('drf_deleteuser/&lt;int:pk&gt;/', drf_views.delete_user, name= 'drf_deleteuser'),\n    path('drf_adduser/', drf_views.user_add_view, name= 'drf_adduser'),\n    path('drf_login/', drf_views.drflogin, name='drf_login'),\n\n    \n]\nWe first create users and then test delete, update and show users functionality of our API. We will use Postman for timebeing. Later we will built the front-end to perform all these actions."
  },
  {
    "objectID": "posts/2021-crud-in-django-rest-framework/index.html#post-request-add-user",
    "href": "posts/2021-crud-in-django-rest-framework/index.html#post-request-add-user",
    "title": "Full Fledged CRUD application using DRF and Token Authentication",
    "section": " POST  REQUEST: ADD USER",
    "text": "POST  REQUEST: ADD USER\nhttp://127.0.0.1:8000/drf_adduser/"
  },
  {
    "objectID": "posts/2021-crud-in-django-rest-framework/index.html#get-request-get-users",
    "href": "posts/2021-crud-in-django-rest-framework/index.html#get-request-get-users",
    "title": "Full Fledged CRUD application using DRF and Token Authentication",
    "section": " GET  REQUEST: GET USERS",
    "text": "GET  REQUEST: GET USERS\nAPI end point\nhttp://127.0.0.1:8000/drf_users/\nUsing curl and passing authorization token\ncurl --location --request GET 'http://127.0.0.1:8000/drf_users/' \\\n--header 'Authorization: Token 92cc8c32edb7bd111b89552a3031f918d2df5613'\nUsing postman"
  },
  {
    "objectID": "posts/2021-crud-in-django-rest-framework/index.html#del-request-delete-user",
    "href": "posts/2021-crud-in-django-rest-framework/index.html#del-request-delete-user",
    "title": "Full Fledged CRUD application using DRF and Token Authentication",
    "section": " DEL  REQUEST: DELETE USER",
    "text": "DEL  REQUEST: DELETE USER\nAPI end point\nhttp://127.0.0.1:8000/drf_deleteuser/&lt;int:pk&gt;\nUsing curl and passing authorization token\ncurl --location --request DELETE 'http://127.0.0.1:8000/drf_deleteuser/21' \\\n--header 'Authorization: Token 1529e77c59999f819649828a5e9174ba44bd6bb4'\nUsing postman"
  },
  {
    "objectID": "posts/2021-crud-in-django-rest-framework/index.html#put-request-update-user",
    "href": "posts/2021-crud-in-django-rest-framework/index.html#put-request-update-user",
    "title": "Full Fledged CRUD application using DRF and Token Authentication",
    "section": " PUT  REQUEST: UPDATE USER",
    "text": "PUT  REQUEST: UPDATE USER\nAPI end point\nhttp://127.0.0.1:8000/drf_updateuser/1/?username=updated_username_here&email=updated_email_here\nUsing curl and passing authorization token\ncurl --location --request PUT 'http://127.0.0.1:8000/drf_updateuser/8/?username=rcbfl&email=rcbfl@gmail.com' \\\n--header 'Authorization: Token 506ce0bbf7fa50f613678024586669d9b6bd82a0'\nusing postman"
  },
  {
    "objectID": "posts/2021-crud-in-django-rest-framework/index.html#get-request-get-user",
    "href": "posts/2021-crud-in-django-rest-framework/index.html#get-request-get-user",
    "title": "Full Fledged CRUD application using DRF and Token Authentication",
    "section": " GET  REQUEST: GET USER",
    "text": "GET  REQUEST: GET USER\nAPI end point\nhttp://127.0.0.1:8000/drf_user/&lt;int:pk&gt;\nUsing curl and passing authorization token\ncurl --location --request GET 'http://127.0.0.1:8000/drf_user/8' \\\n--header 'Authorization: Token 506ce0bbf7fa50f613678024586669d9b6bd82a0'\nusing postman"
  },
  {
    "objectID": "posts/2021-crud-in-django-rest-framework/index.html#front-end-setup",
    "href": "posts/2021-crud-in-django-rest-framework/index.html#front-end-setup",
    "title": "Full Fledged CRUD application using DRF and Token Authentication",
    "section": "Front end setup",
    "text": "Front end setup\nIn root directory create folder templates\\Accountsapp\\ and create RegiserUser.html file in it. Create form field in the file as follows\n          &lt;form class=\"form-horizontal\" action=\"\" method=\"post\"  id=\"myForm\" autocomplete=\"off\"&gt;\n            {% csrf_token %}\n            &lt;!-- Name input--&gt;\n            &lt;div class=\"form-group\"&gt;\n              &lt;label class=\"col-md-3 control-label\" for=\"username\"&gt;Name&lt;/label&gt;\n              &lt;div class=\"col-md-9\"&gt;\n                &lt;input id=\"username\" name=\"username\" type=\"text\" placeholder=\"Your username\" class=\"form-control\"&gt;\n              &lt;/div&gt;\n            &lt;/div&gt;\n            &lt;!-- Email input--&gt;\n            &lt;div class=\"form-group\"&gt;\n              &lt;label class=\"col-md-3 control-label\" for=\"email\"&gt;Your E-mail&lt;/label&gt;\n              &lt;div class=\"col-md-9\"&gt;\n                &lt;input id=\"email\" name=\"email\" type=\"email\" placeholder=\"Your email\" class=\"form-control\"&gt;\n              &lt;/div&gt;\n            &lt;/div&gt;\n            &lt;!-- password body --&gt;\n            &lt;div class=\"form-group\"&gt;\n              &lt;label class=\"col-md-3 control-label\" for=\"password\"&gt;Password&lt;/label&gt;\n              &lt;div class=\"col-md-9\"&gt;\n                &lt;input id=\"password\" name=\"password\" type=\"password\" placeholder=\"Your password\" class=\"form-control\"&gt;\n              &lt;/div&gt;\n            &lt;/div&gt;\n            &lt;!-- password body --&gt;\n            &lt;div class=\"form-group\"&gt;\n              &lt;label class=\"col-md-3 control-label\" for=\"password2\"&gt;Password2&lt;/label&gt;\n              &lt;div class=\"col-md-9\"&gt;\n                &lt;input id=\"password2\" name=\"password2\" type=\"password\" placeholder=\"confirm password\" class=\"form-control\"&gt;\n              &lt;/div&gt;\n            &lt;/div&gt;\n            \n            &lt;!-- superuser input --&gt;\n            &lt;div class=\"form-group\"&gt;\n              &lt;label class=\"col-md-3 control-label\" for=\"superuser\"&gt;Is superuser&lt;/label&gt;\n              &lt;div class=\"col-md-3\"&gt;\n                &lt;input id=\"issuperuser\" name=\"issuperuser\" type=\"checkbox\"  class=\"form-control\" &gt;\n              &lt;/div&gt;\n            &lt;/div&gt;\n \n    \n            &lt;!-- Form actions --&gt;\n            &lt;div class=\"form-group\"&gt;\n              &lt;div class=\"col-md-6 text-left\"&gt;\n                &lt;button type=\"submit\" class=\"btn btn-primary btn-lg\"&gt;Submit&lt;/button&gt;\n              &lt;/div&gt;\n\n\n            &lt;/div&gt;\n          &lt;/fieldset&gt;\n          &lt;/form&gt;\nOnce the form is created, we now need to take the input from the form and send to the register user API drf_adduser/.\nIn RegisterUser.html\n&lt;script type=\"text/javascript\"&gt;\n\n\n            function getCookie(name) {\n            var cookieValue = null;\n            if (document.cookie && document.cookie !== '') {\n                var cookies = document.cookie.split(';');\n                for (var i = 0; i &lt; cookies.length; i++) {\n                    var cookie = cookies[i].trim();\n                    // Does this cookie string begin with the name we want?\n                    if (cookie.substring(0, name.length + 1) === (name + '=')) {\n                        cookieValue = decodeURIComponent(cookie.substring(name.length + 1));\n                        break;\n                    }\n                }\n            }\n            return cookieValue;\n        }\n        var csrftoken = getCookie('csrftoken');\n\n\n\nfunction fetchcall(event) {\n\n        event.preventDefault();\n        console.log('form submitted');\n    var username = document.getElementById(\"username\").value;\n    var email = document.getElementById(\"email\").value;\n    var password = document.getElementById(\"password\").value;\n    var password2 = document.getElementById(\"password2\").value;\n    var issuperuser = document.getElementById(('issuperuser')).checked;\n    console.log(issuperuser)\n\n        var url = '/drf_adduser/';\n\n            fetch(url, {\n                method:'POST',\n                headers:{\n                    'Content-type':'application/json',\n                    'X-CSRFToken':csrftoken,\n                },\n                body:JSON.stringify({\n                    'email':email,\n                    'username':username,\n                    \"password\":password,\n                    \"password2\":password2,\n                    \"is_superuser\": issuperuser\n                })\n            }\n            ).then(function(response){\n                store_response= response;\n                return response.json();\n\n            }).then(function(data){\n                store_data =JSON.stringify(data);\n                document.getElementById(\"message\").innerHTML=  store_data;\n            }).catch(function(error){\n            console.error(error);\n        });\n\n    }\n            \n    var myForm = document.getElementById(\"myForm\");\n\n        console.log(username, password, myForm);\n    myForm.addEventListener('submit', fetchcall);\n    \n&lt;/script&gt;\nTo make this work in front-end, we need to register the file to Accountsapp/views.py\ndef register_user(request):\n    # if request.user.is_authenticated:\n    return render(request, \"Accountsapp/RegisterUser.html\", {'Title': \"Register new user\"})"
  },
  {
    "objectID": "posts/2022-02-07-how-to-save-earth-engine-image-directly-to-your-local-machine/index.html",
    "href": "posts/2022-02-07-how-to-save-earth-engine-image-directly-to-your-local-machine/index.html",
    "title": "How to save Earth Engine Image directly to your local machine",
    "section": "",
    "text": "Oftentimes you are required to download satellite images for your Area of Interest (AOI) and Google Earth Engine is probably a good place to avail processed satellite images for free and more importantly, only for the area you need.\nOne hindrance when you download from earth engine is that the images get saved in google drive, which can fill up fast for large numbers of downloads. To avoid this additional step, there is a hacky trick to download images directly.\nNote: Earth Engine does provide getDownloadURL option, but is limited in the size of download and thus not feasible in this case. Pixel grid dimensions for getDownloadURL must be less than or equal to 10000 i.e, you can have a maximum of 100 x 100 pixel size images.\nIn this post I show a trick which can let you download upto 100 times larger size images, directly to your local machine. Spoiler: getRegion method plays a significant role to help accomplish this task. Added to that, creating a gridded bounding box for our AOI, with spacing equivalent to the pixel size will aid in our task.\nWe will utilize earth engine python client so that all the geopython goodies can be simultaneously utilised.\nTo begin with, I have a geopackage containing a polygon, which is our AOI. We aim to download sentinel-2 B4 band for the region. The ideal way would be to use the in-built Export option, but in our case we would use the getRegion method along with creating a point grid over our AOI with spacing equivalent to the pixel size.\n\n\n\nFig.1 -Left: Our Area of Interest over which to download satellite data. Right: Grid points over AOI bounding box at pixel spacing\n\n\nTo accomplish creation of points at spacing equal to pixel width and height, we use the following function\n# generate points\ndef xcor(y_pt, crs):\n    def wrap(x_each):\n        feat = ee.FeatureCollection(y_pt.map(lambda y_each: ee.Feature(\n            ee.Geometry.Point([x_each, y_each], ee.Projection(crs)))))\n        return feat\n    return wrap\nThe above code can be interpreted as a nested loop.\n# Pseudo code\nfor each_x in x_pt:\n    for each_y in y_pt:\n        create_Point(each_x, each_y)\nx_pt and y_pt are generated from the geopackage (AOI) using GeoPandas library as follows\ndef generatePoints(file_name, pixel_size):\n\n    # read the farm and convert to geojson\n    feature = gpd.read_file(file_name).__geo_interface__\n    # extract bounds\n    minx, miny, maxx, maxy = feature['bbox']\n    # create a list with spacing equal to pixel_size\n    x_pt = ee.List.sequence(minx, maxx, pixel_size)\n    y_pt = ee.List.sequence(miny, maxy, pixel_size)\n   \n    return x_pt, y_pt, minx, maxy\nHere we are basically creating a new Point feature for each x and y point.\nOnce we have the grid over our AOI, we can go ahead and call getRegion method\nThe documentation does a good job in explaining what getRegion is all about\n\nOutput an array of values for each [pixel, band, image] tuple in an ImageCollection. The output contains rows of id, lon, lat, time, and all bands for each image that intersects each pixel in the given region. Attempting to extract more than 1048576 values will result in an error.\n\nThe limit 1048576 results in a max tile width and height of 1024 x 1024 pixels. By combining the previously created grid and getRegion, we could potentially get 100 times more pixels than getDownloadURL. Let us do that!\nlen_y = len(y_pt.getInfo())\nlen_x = len(x_pt.getInfo())\n\nimgCollection = ee.ImageCollection(\"COPERNICUS/S2_SR\").filters(filters_to_add)\ngeometry = ee.FeatureCollection(x_pt.map(xcor(y_pt, CRS))).flatten()\ninput_bands = \"B4\"\npixel_size = 10\n\ndf = get_dataframe(imgCollection, geometry, input_bands, CRS )\ndata_matrix = df[input_bands].values.reshape(len_y, len_x)\ndata_matrix = np.flip(data_matrix, axis = 0)\ntransform = rasterio.transform.from_origin(minx, maxy, pixel_size, pixel_size)\nsave_tiff(\"output.tif\", data_matrix, transform, CRS)\nThe above code first gets the count of points in each of the 2-dimensions followed by fetching the dataframe which contains the lat, lon and pixel value as shown in the image.\n\n\n\nFig.2 -Output of getRegion results in lat, lon, pixel value in sequential order\n\n\nNow we reshape the dataframe and flip it to make the pixel arrange in image format. Lastly, we save the image by passing the transformation of the image. Make sure to have an imageCollection for the getRegion method to work. Currently, the above code can only download 1 band at a time, but with simple modification to the getDataframe function, that too can be changed.\ndef getDataframe(img_col, feature, input_band, crs):\n   \n    imgcol = ee.ImageCollection(img_col).select(input_band)\n    df = pd.DataFrame(imgcol.getRegion(feature.geometry(), 10, crs).getInfo())\n    df, df.columns = df[1:], df.iloc[0]\n    df = df.drop([\"id\", \"time\"], axis=1)\n\n    return df\n\ndef saveTiff(output_name, data_array, transform, crs):\n\n    options = {\n        \"driver\": \"Gtiff\",\n        \"height\": data_array.shape[0],\n        \"width\": data_array.shape[1],\n        \"count\": 1,\n        \"dtype\": np.float32,\n        \"crs\": crs,\n        \"transform\": transform\n    }\n\n    with rs.open(output_name, 'w', **options) as src:\n        src.write(data_array, 1)\n\n    return None\nThe output of the exercise is that you have a raster directly downloaded to your local machine, without google drive/ cloud intermediaries. One thing worth pointing out, is for extremely large images, you are better off downloading via the specified steps in docs. This hacky way is to simplify things and avoid google drive (which is never empty for me).\n\nThe full code can be accessed here"
  },
  {
    "objectID": "posts/2022-07-28-download-modis-data-using-cmr-api-in-python/index.html",
    "href": "posts/2022-07-28-download-modis-data-using-cmr-api-in-python/index.html",
    "title": "Download MODIS data using CMR API in Python",
    "section": "",
    "text": "If you have ever used USGS Earth Explorer to download / explore data, you’d notice that the manual process is cumbersome and not scalable. That is why we require a programmatic way to download satellite data.\nIn this blog we’d see how to download MODIS data using Python. We use a Python package called modis-tools to perform our task. This package internally uses NASA CMR (Common Metadata Repository) API which lets us search and query catalogs of various satellite dataset including MODIS.\nWe focus on the MODIS dataset in this blog, but with little modification, we could extend for various other datasets.\nBefore you move ahead, make sure you have an earthdata account. We would require the username and password to download the data. Register here if not done so.\n\nTo download the data we ask ourselves the following questions:\n\nWhich dataset specifically do I need? — Define Dataset Name\nWhat area do I need the data for? — Define our Region of Interest\nWhat time period of data do I require? — Define Start and End Date\n\nHere, I wish to download MODIS Surface Reflectance 8-Day L3 Global 250 m SIN Grid data for Nigeria from 29 December, 2019 to 31st December, 2019.\nLet us install and use the Python package modis-tools to download the data on our local machine by performing the following steps\n\nCreate a virtual environment.\nInstall the modis-tools package.\nWrite the code.\n\n\nTo create a new environment\nCreate virtual environment .modis-tools using Python’s venv\naman@AMAN-JAIN:~$ python3 -m venv .modis-tools\nActivate the environment.\naman@AMAN-JAIN:~$ source .modis-tools/bin/activate\nNote: The above command is for linux. For Windows use .modis-tools\\Scripts\\activate instead.\n\n\nInstall the modis-tools package\n(.modis-tools) aman@AMAN-JAIN:~$ pip install modis-tools\n\n\nInsert the below code\nPaste the code in a python file named download.py.\n# download_modis.py\n\n# 1) connect to earthdata\nsession = ModisSession(username=username, password=password)\n\n# 2) Query the MODIS catalog for collections\ncollection_client = CollectionApi(session=session)\ncollections = collection_client.query(short_name=\"MOD09GQ\", version=\"061\")\n# Query the selected collection for granules\ngranule_client = GranuleApi.from_collection(collections[0], session=session)\n\n# 3) Filter the selected granules via spatial and temporal parameters\nnigeria_bbox = [2.1448863675, 3.002583177, 4.289420717, 4.275061098] # format [x_min, y_min, x_max, y_max]\nnigeria_granules = granule_client.query(start_date=\"2019-12-29\", end_date=\"2019-12-31\", bounding_box=nigeria_bbox)\n\n# 4) Download the granules\nGranuleHandler.download_from_granules(nigeria_granules, session, threads=-1)\nIn the above code, change the username, password, nigeria_box and start_date & end_date according to your requirements.\nTo explain the above code —\n\nFirst we create a session, which makes a connection to earthdata and registers a session. Next three lines we search for MODIS Surface Reflectance 8-Day L3 Global 250 m SIN Grid dataset using short_name and version.\nNow we filter the region spatially and temporally we want our data to be downloaded. In this example, we filter for the nigeria region with a bounding box (bounding_box) and the two days of december of 2019 (start_date, end_date).\nLastly, we download the data (granules) using multithreading, since we asked to use all threads. (threads=-1 is all threads).\n\n\n\nHow to get short_name and version for the dataset?\nThe collection endpoint of the CMR API contains a directory of all dataset catalogs hosted by various organizations with its short name and version number. For MODIS data, LPDAAC_ECS hosts and maintains it. Under the /collections/directory endpoint, look for LPDAAC_ECS and search for the MODIS dataset you want to download. Each dataset has a short name and version associated with it as shown in the picture below. In our case we found MOD09Q1 short name with version 061.\n\n\nNow it is time to run the code to see our data being downloaded.\nIn your terminal, run —\n(.modis-tools) aman@AMAN-JAIN:~$ python download_modis.py\nDownloading: 100%|██████████████████████████████████████████████████████| 3/3 [00:10&lt;00:00,  3.67s/file]\nA progress bar would let you see the download progress and the files would be downloaded to your local disk. If you wish to download the data to a specific directory, use the path parameter in download_from_granules classmethod.\nEndnote This short post on downloading MODIS data originated when I wanted to set up and deploy a pipeline. I did find other packages but they were quite old and did not use the state of the art specifications. Since the solution presented here uses CMR API, which has a very good documentation, I preferred it over other tools.\nYou can find the video version of this blog here\n\n\n\nFor the curious (Advanced)\nThe base url for the CMR API is —\nhttps://cmr.earthdata.nasa.gov/search\nInternally, CMR API first finds the collection for our dataset —\nhttps://cmr.earthdata.nasa.gov/search/collections.json?short_name=MOD09GQ&version=061\nAfter that the package queries the granules endpoint to find individual granules matching our query parameters —\nhttps://cmr.earthdata.nasa.gov/search/granules.json?downloadable=true&scroll=true&page_size=2000&sort_key=-start_date&concept_id=C1621091662-LPDAAC_ECS&temporal=2019-12-01T00%3A00%3A00Z%2C2019-12-31T00%3A00%3A00Z&bounding_box=2.1448863675%2C3.002583177%2C4.289420717%2C4.275061098\nNote that most parameters are autogenerated by the python package depending on the short_name and version you provide (downloadable, scroll, page_size, sort_key, concept_id). The other parameters are user defined (temporal, bounding_box)\nThere are many more additional parameters which can be passed. A complete list is present in the documentation. One such useful parameter that you can try out is cloud_cover. All you need to do is pass this parameter name with value to the query method in the above code."
  },
  {
    "objectID": "posts/2022-09-18-import-csv-and-osm-data-into-postgresql-using-ogr2ogr/index.html",
    "href": "posts/2022-09-18-import-csv-and-osm-data-into-postgresql-using-ogr2ogr/index.html",
    "title": "Import CSV and OSM data into PostgreSQL using ogr2ogr",
    "section": "",
    "text": "ogr2ogr is the swiss knife for vector geometry conversion. You can import CSV with latitude and longitude columns as Point geometry into PostgreSQL. This tool also makes it easy to import OSM data to be imported into PostgreSQL with a lot of flexibility."
  },
  {
    "objectID": "posts/2022-09-18-import-csv-and-osm-data-into-postgresql-using-ogr2ogr/index.html#insert-csv-to-postgresql",
    "href": "posts/2022-09-18-import-csv-and-osm-data-into-postgresql-using-ogr2ogr/index.html#insert-csv-to-postgresql",
    "title": "Import CSV and OSM data into PostgreSQL using ogr2ogr",
    "section": "1. Insert CSV to PostgreSQL",
    "text": "1. Insert CSV to PostgreSQL\nOur CSV contains information about retail food stores including cafes, restaurants, grocery information with the location and name. Download the data here\n\n\n\nimage\n\n\nWe first read the metadata of the CSV using ogrinfo\nogrinfo -so filter_all_cat_data.csv filter_all_cat_data\nAssuming you have a database already (postgres here), we run the following command to create postgis extension for postgres database. The connection string is of the format as described here\npsql -c \"create extension postgis;\" \"postgresql://postgres:1234@localhost:5432/postgres\"\nFinally, we insert the CSV into PostgreSQL table named cat_data_copy and assign CRS of EPSG:4326.\nogr2ogr -f PostgreSQL PG:\"host=localhost user=postgres dbname=postgres password=1234\" filter_all_cat_data.csv -oo X_POSSIBLE_NAMES=long_url -oo Y_POSSIBLE_NAMES=lat_url -nlt POINT -nln \"cat_data_copy\" -sql \"select name,city,lat_url,long_url,type from filter_all_cat_data\" -a_srs \"EPSG:4326”\nThe following explains few of the flags\n\n-oo: X_POSSIBLE_NAMES and Y_POSSIBLE_NAMES allows us to specify geometry columns from CSV\n-nlt: Define the geometry type for the table\n-nln: alternate Table name (defaults to name of the file)\n-sql: write SQL to insert only selected columns into the table"
  },
  {
    "objectID": "posts/2022-09-18-import-csv-and-osm-data-into-postgresql-using-ogr2ogr/index.html#insert-osm-data-to-postgresql",
    "href": "posts/2022-09-18-import-csv-and-osm-data-into-postgresql-using-ogr2ogr/index.html#insert-osm-data-to-postgresql",
    "title": "Import CSV and OSM data into PostgreSQL using ogr2ogr",
    "section": "2. Insert OSM data to PostgreSQL",
    "text": "2. Insert OSM data to PostgreSQL\nOur OSM data is of Bahamas downloaded from geofabrik. You can download it from here\nWe first read the metadata of the OSM data using ogrinfo\nogrinfo -so bahamas-latest.osm.pbf multipolygons\nWe find about the geometry column, CRS and columns in the data. This will be used when inserting the data into the database.\nNext we create postgis and hstore extensions in our postgres database.\npsql -c \"create extension hstore; create extension postgis\" \"postgresql://postgres:1234@localhost:5432/postgres\"\nFinally we insert the data into PostgreSQL with table name as bahamas_mpoly with only multipolygons. We convert the other_tags column into hstore and insert only those rows where the name column does not contain a null value. We also clip our data to a bounding box and promote our polygons to multipolygons to avoid error.\nogr2ogr -f PostgreSQL PG:\"dbname=postgres host=localhost port=5432 user=postgres password=1234\" bahamas-latest.osm.pbf multipolygons -nln bahamas_mpoly -lco COLUMN_TYPES=other_tags=hstore -overwrite -skipfailures -where \"name is not null\" -clipsrc -78 23 -73 27 -nlt PROMOTE_TO_MULTI\n\n\nVideo version of the blog can be found here"
  },
  {
    "objectID": "posts/2023-01-14-cloud-native-composite-subset-and-processing-of-satellite-imagery-with-stac-and-stackstac/index.html",
    "href": "posts/2023-01-14-cloud-native-composite-subset-and-processing-of-satellite-imagery-with-stac-and-stackstac/index.html",
    "title": "Cloud Native Composite, Subset and Processing of Satellite Imagery with STAC and Stackstac",
    "section": "",
    "text": "If you wanted to collect all Sentinel satellite data for a given region of interest (ROI), say, for a given day or time frame - is there any simple way to do it? That means: Without having to download all the full images manually and cropping the ROI subset manually as well afterwards?\n\nThis, well articulated question, was the one which I was facing and made me ponder to think if we could do this using STAC and Python.\n\nI had a road network layer over which I needed satellite imagery. The problem with my road network is that it has a large spatial extent, causing a single satellite imagery to not cover it entirely. Moreover, because of this large extent, I need two adjacent tiles to be in the same Coordinate Reference System.\n\n\n\nFig.1 - Road network (in red) spanning multiple UTM Zones. Basemap from OSM.\n\n\nWhat I needed was, - A way to aggregate all the adjacent tiles for a single day - Convert to a single CRS on the fly - Subset the data to my region - Create a composite (merge) and perform analysis on the fly\nIt turns out Python (and its ecosystem of great geospatial packages) along with STAC allows us to do just that.\nWhat is STAC? &gt; STAC (SpatioTemporal Asset Catalog) is an open-source specification for describing satellite imagery and the associated metadata.\nWe will use stackstac, which is a Python package for efficiently managing and analysing large amounts of satellite imagery data in a cloud computing environment.\nFirst, we search through the sentinel-2 collection for our area of interest from element84 provided STAC endpoint.\nfrom pystac_client import Client\n\nURL = 'https://earth-search.aws.element84.com/v0/'\n\nclient = Client.open(URL)\nsearch = client.search(\n    max_items = 10,\n    collections = \"sentinel-s2-l2a-cogs\",\n    intersects = aoi_as_multiline,\n    datetime = '2022-01-01/2022-01-24'\n)\nThe resultant search object is passed to stack method on stackstac along with providing the destination CRS, the region of bounds and the assets required.\nimport stackstac\n\nds = stackstac.stack(search.get_all_items() ,  epsg=4326, assets=[\"B04\", \"B03\", \"B05\"],\nbounds_latlon= aoi_as_multiline.bounds )\n\nThe above line does a lot of things under the hood. It transforms the CRS of each tile from their native CRS to EPSG:4326. It also clips the tiles to our AOI. It also filters only 3 bands out of the possible 15 sentinel-2 bands. The output ds is a xarray.DataArray object and it is a known fact how much is possible with very little code in xarray.\nAs such, we can group by a date and mosaic those tiles very easily using xarray as shown below.\ndsf = ds.groupby(\"time.date\").median()\n\n\n\nFig.2 - Our DataArray is 3.37GB with 4 dimensions (time, bands, x, y) respectively.\n\n\nSince xarray loads lazily, we did not perform any computation so far. But we can see how much data we are going to end up storing as shown in Figure 2.\nWhen I run the compute method on the output, it does the computation in 4 minutes (here) i.e, processing ~3.5GB in 4 mins and computing the median across the dates.\nres = dsf.compute()\nAt the end of this process, I have 4 images for each of the 4 dates, clipped to my region of interest in the CRS that I desire.\n\nThe above method of processing large volume data is super handy and can be scaled very easily with cloud infrastructure. What is unique about this approach is that I did not have to download data, convert or know the CRS of each tile, worrying about the bounds of my region of interest. Read more about how stackstac works here.\nThe code can be found here."
  },
  {
    "objectID": "posts/gpm-imerg-xarray/index.html",
    "href": "posts/gpm-imerg-xarray/index.html",
    "title": "Download and preprocess NASA GPM IMERG Data using Python and wget",
    "section": "",
    "text": "We are going to work with GPM IMERG Late Precipitation L3 Half Hourly 0.1 degree x 0.1 degree V06 (GPM_3IMERGHHL) data provided by NASA which gives half-hourly precipitation values for entire globe.\n\nPre-requisites\n\nYou must have an Earthdata Account\nLink GES DISC with your account\n\nRefer to this page on how to Link GES DISC to your account.\nFirst method - We would be downloading netCDF data using the requests module and preprocessing the file using xarray.\nSecond method - To download netCDF file using wget and using xarray to preprocess and visualise the data.\n\n\nDownloading link list\nWe first select the region for which we want to download the data by visiting the GPM IMERG website and clicking on subset/ Get Data link at right corner. \nIn the popup, select 1. Download Method as Get File Subsets using OPeNDAP 2. Refine Date Range as the date you want the data for. In my case, I choose 10 days of data. 3. Refine Region to subset data for your area of interest. In my case I choose 77.45,12.85,77.75,13.10 4. Under Variables, select precipitationCal. 5. For file format, we choose netCDF and click the Get Data button.\n\nThis will download a text file, containing all the links to download individual half hourly data for our area of interest in netCDF file format.\nNow we move to Google Colaboratory, to download the data in netCDF file format. We use Google Colaboratory as it has many libraries pre-loaded and saves the hassle to install them.\nIf you’re area of interest (or) the timeframe of download is large, please use local machine as Google Colaboratory only offers ~60 GB of free storage.\n\n\nMethod 1: Using Python to read and preprocess the data inside Google Colaboratory.\nOpen a new google Colab notebook and upload the downloaded text file. Our uploaded text file looks like the following.\n\nAs one last requirement, NASA requires authentication to access the data and thus we have to create a .netrc file and save it at specified location (under /root dir in our case).\n\n\nCreating .netrc file\nOpen your notepad and type in the following text. Make sure to replace your_login_username and your_password with your earthdata credentials. Now save it as .netrc file.\nmachine urs.earthdata.nasa.gov login your_login_username password your_password\nUpload the .netrc file to Colab under root directory as shown in the figure below.\n\nNow we have all the setup done and are ready to code.\nWe first load the required libraries. Then, read the text file and loop over every line in it to download from the URL using the requests module. Finally, we save the file to Colab’s hard drive. If you do not see the files after running code, make sure to wait for at least a day after registering to earthdata to make your account activated. I was late to read about it and had wasted a long time debugging it.\nimport pandas as pd\nimport numpy as np\nimport xarray as xr\nimport requests \n\n# dataframe to read the text file which contains all the download links\nds = pd.read_csv('/content/subset_GPM_3IMERGHH_06_20210611_142330.txt', header = None, sep = '\\n')[0]\n\n# Do not forget to add .netrc file in the root dir of Colab. printing `result` should return status code 200\nfor file in range(2, len(ds)): # skip first 2 rows as they contain metadata files\n  URL = ds[file]\n  result = requests.get(URL)\n  try:\n    result.raise_for_status()\n    filename = 'test' + str(file) + '.nc'\n    with open(filename, 'wb') as f:\n        f.write(result.content)\n\n  except:\n    print('requests.get() returned an error code '+str(result.status_code))\n\nxr_df = xr.open_mfdataset('test*.nc')\n\nxr_df.mean(dim = ['lat', 'lon']).to_dataframe().to_csv('results.csv')\nIn the above snippet, what is interesting is the method open_mfdataset which takes in all the netCDF files and gives us a nice, compact output from which we can subset and further process our data. Here, we take the average of all the values (precipitation) and convert it into a new dataframe. We are ready to export it as CSV.\n\n\nMethod 2: Using wget to download and then preprocess using xarray\nIn this method, we download all the netCDF files using wget. These files are then read using xarray which makes it really easy to process and get the information we require.\nRunning the following shell command in Google Colab will download all the data from the text file URLs. Make sure to replace your_user_name , &lt;url text file&gt; within the command. It will ask for password of your earthdata account on running the cell.\n! wget --load-cookies /.urs_cookies --save-cookies /root/.urs_cookies --auth-no-challenge=on --user=your_user_name --ask-password --content-disposition -i &lt;url text file&gt;\nOnce the above shell command is run on Colab, the following 2 lines of code will give a nice dataframe which can be exported to csv for further analysis.\nimport xarray as xr\nimport glob\n\nds = xr.open_mfdataset('test*.nc')\nds.precipitationCal.mean(dim=('lon', 'lat')).plot() # calculate the average precipitation on a half-hourly basis.\n\n\nFinal Comments\nIn this post we looked into how to download and preprocess netCDF data provided by NASA GES DISC. We looked at two methods, one with pure Python and the other with wget and xarray. All performed on google Colab. It is to be noted that, there is a significant setup required i.e, to create a new .netrc file and store inside the root directory of Colab else it returns an authorisation error. We looked at how easy it is to process netCDF data in xarray and how wget commands can be run on Colab.\nWatch the video tutorial here. The notebook for reference is located here."
  },
  {
    "objectID": "posts/reverse-geocode-using-mapbox-api-with-zoom-functionality/index.html",
    "href": "posts/reverse-geocode-using-mapbox-api-with-zoom-functionality/index.html",
    "title": "Geocoding using Mapbox API with Zoom-in map functionality",
    "section": "",
    "text": "The big picture of this post can be related to google maps, wherein you type the address and it zooms in to the location of interest. We replicate this exact functionality with mapbox API for geocoding and openlayers for client side zoom to the address of interest."
  },
  {
    "objectID": "posts/reverse-geocode-using-mapbox-api-with-zoom-functionality/index.html#overview",
    "href": "posts/reverse-geocode-using-mapbox-api-with-zoom-functionality/index.html#overview",
    "title": "Geocoding using Mapbox API with Zoom-in map functionality",
    "section": "",
    "text": "The big picture of this post can be related to google maps, wherein you type the address and it zooms in to the location of interest. We replicate this exact functionality with mapbox API for geocoding and openlayers for client side zoom to the address of interest."
  },
  {
    "objectID": "posts/reverse-geocode-using-mapbox-api-with-zoom-functionality/index.html#main-steps",
    "href": "posts/reverse-geocode-using-mapbox-api-with-zoom-functionality/index.html#main-steps",
    "title": "Geocoding using Mapbox API with Zoom-in map functionality",
    "section": "Main steps",
    "text": "Main steps\nThis blog demonstrates how to geocode an address using mapbox api implemented in openlayers v6. Additionally zoom in to the search location as text provided on the search bar. This one page appication demostrates only key elements, rest of the customisation is at discretion of the viewer.\n\nSetup the project\nWe first create basic single html file to include all elements (javascript, css and html). Ideally, when the application scales, you would create a seperate file for each component. 1. Create html file and add basic elements\n&lt;html&gt;\n&lt;head&gt;\n&lt;meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\"&gt;\n &lt;link rel=\"stylesheet\" href=\"https://cdn.jsdelivr.net/gh/openlayers/openlayers.github.io@master/en/v6.5.0/css/ol.css\" type=\"text/css\"&gt;\n &lt;style type=\"text/css\"&gt;\n.autocomplete {\n  position: relative;\n  display: inline-block;\n}\ninput {\n  border: 1px solid transparent;\n  background-color: #f1f1f1;\n  padding: 10px;\n  font-size: 16px;\n}\ninput[type=text] {\n  background-color: #f1f1f1;\n  width: 100%;\n}\ninput[type=submit] {\n  background-color: DodgerBlue;\n  color: #fff;\n  cursor: pointer;\n}\n &lt;/style&gt;\n &lt;/head&gt;\n &lt;body&gt;\n&lt;!--create search bar for geocoding and style it --&gt;\n&lt;h2&gt;Autocomplete&lt;/h2&gt;\n&lt;br&gt;\n&lt;form  method=\"post\" &gt;\n  &lt;div class=\"autocomplete\" style=\"width:300px;\"&gt;\n    &lt;input id=\"myInput\" type=\"text\" name=\"myCountry\" placeholder=\"Country\"&gt;\n  &lt;/div&gt;\n  &lt;input type=\"submit\" id = \"geocodingSubmit\"&gt;\n&lt;/form&gt;\n&lt;div id='project_map', class=\"map\"&gt;&lt;/div&gt;\n&lt;/body&gt;\n&lt;script src=\"https://cdn.jsdelivr.net/gh/openlayers/openlayers.github.io@master/en/v6.5.0/build/ol.js\"&gt;&lt;/script&gt;\n&lt;!-- &lt;script src=\"https://cdnjs.cloudflare.com/ajax/libs/proj4js/2.5.0/proj4.js\"&gt;&lt;/script&gt; --&gt;\n&lt;script type=\"text/javascript\"&gt;\n     \n     // create basemap layer\n     var project_maplayer = new ol.layer.Tile({\n    // source: new ol.source.OSM(),\n    source: new ol.source.XYZ({\n        attributions: ['Powered by Esri',\n                                     'Source: Esri, DigitalGlobe, GeoEye, Earthstar Geographics, CNES/Airbus DS, USDA, USGS, AeroGRID, IGN, and the GIS User Community'],\n        attributionsCollapsible: false,\n        url: 'https://services.arcgisonline.com/ArcGIS/rest/services/World_Imagery/MapServer/tile/{z}/{y}/{x}',\n        maxZoom: 23\n    }),\n    zIndex: 0\n});\n\n// create view for the layer\nvar project_view = new ol.View({\n    projection: 'EPSG:4326',\n    center: [-81.80808208706726, 27.285095000261222],\n    zoom: 7,\n});\n\n// add the basemap to the map\nvar Projectmap = new ol.Map({\n    layers: [project_maplayer,],\n    target: 'project_map',\n    view: project_view,\n    constrainOnlyCenter: true,\n});\n&lt;/script&gt;\nWe added the following elements, 1. Search bar: we setup the search function to input values as address and wrap it within a form with post request. 2. Map : the div element with id=\"project_map\" holds the map element and the script does the following. First, create layer with ESRI basemap. Second, add the layer to the Map object.\nAt this stage the application looks like the following image"
  },
  {
    "objectID": "posts/reverse-geocode-using-mapbox-api-with-zoom-functionality/index.html#add-autocomplete-functionality",
    "href": "posts/reverse-geocode-using-mapbox-api-with-zoom-functionality/index.html#add-autocomplete-functionality",
    "title": "Geocoding using Mapbox API with Zoom-in map functionality",
    "section": "Add autocomplete functionality",
    "text": "Add autocomplete functionality\nWe fetch from the api and populate our top results in a list format on key press. Also, we style the search bar using css.\n&lt;style&gt;\n.autocomplete-items {\n  position: absolute;\n  border: 1px solid #d4d4d4;\n  border-bottom: none;\n  border-top: none;\n  z-index: 99;\n  /*position the autocomplete items to be the same width as the container:*/\n  top: 100%;\n  left: 0;\n  right: 0;\n}\n\n.autocomplete-items div {\n  padding: 10px;\n  cursor: pointer;\n  background-color: #fff; \n  border-bottom: 1px solid #d4d4d4; \n}\n\n/*when hovering an item:*/\n.autocomplete-items div:hover {\n  background-color: #e9e9e9; \n}\n\n/*when navigating through the items using the arrow keys:*/\n.autocomplete-active {\n  background-color: DodgerBlue !important; \n  color: #ffffff; \n}\n&lt;/style&gt;\n\n&lt;script&gt;\nmyHeaders =  {'Content-Type': 'application/json', 'Access-Control-Allow-Credentials' : true,\n                    'Access-Control-Allow-Origin':'*',\n                    'Accept': 'application/json'}\n\nfunction autocomplete(inp) {\n  /*the autocomplete function takes one argument,\n  the text field element*/\n  var currentFocus;\n  /*execute a function when someone writes in the text field:*/\n  inp.addEventListener(\"input\", function(e) {\n      var a, b, i, val = this.value;\n      var ACCESS_TOKEN_KEY = 'your_token_here'\n      /*close any already open lists of autocompleted values*/\n      var URL = `https://api.mapbox.com/geocoding/v5/mapbox.places/${val}.json?access_token=${ACCESS_TOKEN_KEY}&types=address,region,poi,country,district,locality,neighborhood,postcode&country=us`\n     \n      fetch(URL,{\n        method: 'GET',\n        headers: myHeaders,\n      }).then(response =&gt; response.json())\n      .then(data =&gt; {\n        geocode_data = data;\n        // console.log(data) \n      \n      closeAllLists();\n      if (!val) { return false;}\n      currentFocus = -1;\n      /*create a DIV element that will contain the items (values):*/\n      a = document.createElement(\"DIV\");\n      a.setAttribute(\"id\", this.id + \"autocomplete-list\");\n      a.setAttribute(\"class\", \"autocomplete-items\");\n      /*append the DIV element as a child of the autocomplete container:*/\n      this.parentNode.appendChild(a);\n      /*for each item in the array...*/\n      for (i = 0; i &lt; geocode_data.features.length; i++) {\n\n          b = document.createElement(\"DIV\");\n          /*insert a input field that will hold the current array item's value:*/\n          b.innerHTML += geocode_data.features[i].place_name;\n          b.innerHTML += `&lt;input type='hidden' style=\"display: none;\" id=${i}-center-cc  \n          coordinates='${geocode_data.features[i].center}' value='${geocode_data.features[i].place_name}'&gt;`;\n          \n          /*execute a function when someone clicks on the item value (DIV element):*/\n          b.addEventListener(\"click\", function(e) {\n              /*insert the value for the autocomplete text field:*/\n              var input_tag = this.getElementsByTagName(\"input\")[0]\n              inp.value = input_tag.value;\n              inp.setAttribute(\"coordinates\", input_tag.getAttribute('coordinates'));\n\n              /*close the list of autocompleted values,\n              (or any other open lists of autocompleted values:*/\n              closeAllLists();\n          });\n          a.appendChild(b);\n        }\n\n      })\n      .catch(error =&gt; {\n    console.error('There has been a problem with your fetch operation:', error);\n    });\n\n\n      });\n  // });\n  /*execute a function presses a key on the keyboard:*/\n  inp.addEventListener(\"keydown\", function(e) {\n      var x = document.getElementById(this.id + \"autocomplete-list\");\n      if (x) x = x.getElementsByTagName(\"div\");\n      if (e.keyCode == 40) {\n        /*If the arrow DOWN key is pressed,\n        increase the currentFocus variable:*/\n        currentFocus++;\n        /*and and make the current item more visible:*/\n        addActive(x);\n      } else if (e.keyCode == 38) { //up\n        /*If the arrow UP key is pressed,\n        decrease the currentFocus variable:*/\n        currentFocus--;\n        /*and and make the current item more visible:*/\n        addActive(x);\n      } else if (e.keyCode == 13) {\n        /*If the ENTER key is pressed, prevent the form from being submitted,*/\n        e.preventDefault();\n        if (currentFocus &gt; -1) {\n          /*and simulate a click on the \"active\" item:*/\n          if (x) x[currentFocus].click();\n        }\n      }\n  });\n  function addActive(x) {\n    /*a function to classify an item as \"active\":*/\n    if (!x) return false;\n    /*start by removing the \"active\" class on all items:*/\n    removeActive(x);\n    if (currentFocus &gt;= x.length) currentFocus = 0;\n    if (currentFocus &lt; 0) currentFocus = (x.length - 1);\n    /*add class \"autocomplete-active\":*/\n    x[currentFocus].classList.add(\"autocomplete-active\");\n  }\n  function removeActive(x) {\n    /*a function to remove the \"active\" class from all autocomplete items:*/\n    for (var i = 0; i &lt; x.length; i++) {\n      x[i].classList.remove(\"autocomplete-active\");\n    }\n  }\n  function closeAllLists(elmnt) {\n    /*close all autocomplete lists in the document,\n    except the one passed as an argument:*/\n    var x = document.getElementsByClassName(\"autocomplete-items\");\n    for (var i = 0; i &lt; x.length; i++) {\n      if (elmnt != x[i] && elmnt != inp) {\n        x[i].parentNode.removeChild(x[i]);\n      }\n    }\n  }\n  /*execute a function when someone clicks in the document:*/\n  document.addEventListener(\"click\", function (e) {\n      closeAllLists(e.target);\n  });\n}\n\n/*initiate the autocomplete function on the \"myInput\" element */\nautocomplete(document.getElementById(\"myInput\"));\n\n&lt;/script&gt;\nThe following is the explanation of the code\n\nautocomplete function: The function takes an element as input which needs to be populated. Then we add an event listner which on change in input field, triggers. A GET request is sent across for the input typed and the result is populated in a form of dropdown. We add some styling on key-down so as to select the search.\n\nAt this point, with correct mapbox api access key, we have built the autocomplete functionality."
  },
  {
    "objectID": "posts/reverse-geocode-using-mapbox-api-with-zoom-functionality/index.html#last-steps",
    "href": "posts/reverse-geocode-using-mapbox-api-with-zoom-functionality/index.html#last-steps",
    "title": "Geocoding using Mapbox API with Zoom-in map functionality",
    "section": "Last steps",
    "text": "Last steps\nWe now only need to implement the submit functionality. On click of submit button, the address is located on the map and zoomed in. This is done using a function we call centerMap\nfunction CenterMap() {\n    var [long, lat] = document.getElementById(\"myInput\").getAttribute(\"coordinates\").split(\",\").map(Number)\n    console.log(\"Long: \" + long + \" Lat: \" + lat);\n    Projectmap.getView().setCenter(ol.proj.transform([long, lat], 'EPSG:4326', 'EPSG:4326'));\n    Projectmap.getView().setZoom(5);\n}\nNow we add the centerMap function on click of submit\ndocument.getElementById(\"geocodingSubmit\").addEventListener('click', function(e){\n\n    e.preventDefault();\n    CenterMap()\n})\nThe associated running application can be found here"
  },
  {
    "objectID": "presentations/asce_conference_2019/index.html",
    "href": "presentations/asce_conference_2019/index.html",
    "title": "Structural Modelling and analysis of concrete canoe hull",
    "section": "",
    "text": "Hello hello world"
  },
  {
    "objectID": "presentations/design_of_surface_drainage_2021/index.html",
    "href": "presentations/design_of_surface_drainage_2021/index.html",
    "title": "Structural Modelling and analysis of concrete canoe hull",
    "section": "",
    "text": "Hello hello world"
  },
  {
    "objectID": "presentations/index.html",
    "href": "presentations/index.html",
    "title": "Presentations",
    "section": "",
    "text": "Order By\n      Default\n      \n        Title\n      \n      \n        Date - Oldest\n      \n      \n        Date - Newest\n      \n      \n        Author\n      \n    \n  \n\n\n\n\n\n\n\n\n\n\nFeasibility Study of Floating Solar Panels over Lakes in Bengaluru City\n\n\n\n\n\nJul 28, 2019\n\n\n\n\n\n\n\n\n\n\n\n\nMechanistic Approach for the Design of Surface Drainage System of Roads\n\n\n\n\n\nSep 8, 2021\n\n\n\n\n\n\n\n\n\n\n\n\nData Jam Presentation organised by WRI and opencity\n\n\n\n\n\nJun 10, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nSentinel-1 Image Analysis: Flood Detection using Cloud Native Tooling\n\n\n\n\n\nNov 30, 2023\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "talks/index.html#planetary-computer-and-stac-api",
    "href": "talks/index.html#planetary-computer-and-stac-api",
    "title": "Talks",
    "section": "Planetary Computer and STAC API",
    "text": "Planetary Computer and STAC API"
  },
  {
    "objectID": "talks/index.html#geopython",
    "href": "talks/index.html#geopython",
    "title": "Talks",
    "section": "Geopython",
    "text": "Geopython"
  },
  {
    "objectID": "talks/index.html#event-moderation-and-hosting",
    "href": "talks/index.html#event-moderation-and-hosting",
    "title": "Talks",
    "section": "Event Moderation and Hosting",
    "text": "Event Moderation and Hosting"
  },
  {
    "objectID": "talks/index.html#qgis",
    "href": "talks/index.html#qgis",
    "title": "Talks",
    "section": "QGIS",
    "text": "QGIS"
  },
  {
    "objectID": "events/index.html#june-2025",
    "href": "events/index.html#june-2025",
    "title": "Events",
    "section": "June 2025",
    "text": "June 2025\nVisited varkala for the first time. An absolute delight to be at south/north cliff with an amazing company."
  },
  {
    "objectID": "events/index.html#august-2025",
    "href": "events/index.html#august-2025",
    "title": "Events",
    "section": "",
    "text": "Had a workshop session for NITK on python data handling and mining. You can find the slides here. The associated github code is here"
  }
]